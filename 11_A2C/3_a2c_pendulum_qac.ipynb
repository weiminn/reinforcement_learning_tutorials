{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Augment as Q Actor-Critic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import copy\n",
        "import torch\n",
        "import random\n",
        "import gym\n",
        "import matplotlib\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import deque, namedtuple\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import IterableDataset\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from torch.distributions import Normal\n",
        "\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "\n",
        "from gym.wrappers import RecordVideo, RecordEpisodeStatistics, NormalizeObservation\n",
        "\n",
        "# For visualization\n",
        "from gym.wrappers.monitoring import video_recorder\n",
        "from IPython.display import HTML\n",
        "from IPython import display \n",
        "import glob\n",
        "\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "num_gpus = torch.cuda.device_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_env(env_name, num_envs):\n",
        "  env = gym.vector.make(env_name, num_envs)\n",
        "  env = RecordEpisodeStatistics(env)\n",
        "  env = NormalizeObservation(env)\n",
        "  return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(Actor, self).__init__()\n",
        "        self.linear1 = nn.Linear(state_size, 32)\n",
        "        self.linear2 = nn.Linear(32, 32)\n",
        "        self.linear3 = nn.Linear(32, action_size)\n",
        "        self.reset_parameters()\n",
        "        \n",
        "    def reset_parameters(self):\n",
        "        self.linear1.weight.data.normal_(0, 1e-1)\n",
        "        self.linear2.weight.data.normal_(0, 1e-1)\n",
        "        self.linear3.weight.data.normal_(0, 1e-2)\n",
        "    \n",
        "    def forward(self, state):\n",
        "        x = state\n",
        "        x = self.linear1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.linear3(x)\n",
        "        return torch.tanh(x)\n",
        "    \n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(Critic, self).__init__()\n",
        "        self.linear1 = nn.Linear(state_size + action_size, 64)\n",
        "        self.linear2 = nn.Linear(64, 64)\n",
        "        self.linear3 = nn.Linear(64, 1)\n",
        "        self.reset_parameters()\n",
        "        \n",
        "    def reset_parameters(self):\n",
        "        self.linear1.weight.data.normal_(0, 1e-1)\n",
        "        self.linear2.weight.data.normal_(0, 1e-1)\n",
        "        self.linear3.weight.data.normal_(0, 1e-2)\n",
        "    \n",
        "    def forward(self, state, action):\n",
        "        x = torch.cat((state, action), dim=1)        \n",
        "        x = self.linear1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.linear3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class OUActionNoise():\n",
        "    def __init__(self, mu, sigma=0.15, theta=0.2, dt=1e-2, x0=None):\n",
        "        self.theta = theta\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "        self.dt = dt\n",
        "        self.x0 = x0\n",
        "        self.reset()\n",
        "\n",
        "    def __call__(self):\n",
        "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
        "                self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
        "        self.x_prev = x\n",
        "\n",
        "        return x\n",
        "\n",
        "    def reset(self):\n",
        "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Memory:\n",
        "    def __init__(self, buffer_size, batch_size):\n",
        "        self.buffer_size = buffer_size\n",
        "        self.batch_size = batch_size\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, element):\n",
        "        if len(self.memory) < self.buffer_size:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = element\n",
        "        self.position = (self.position + 1) % self.buffer_size\n",
        "\n",
        "    def sample(self):\n",
        "        return list(zip(*random.sample(self.memory, self.batch_size)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "bBQmph67MUru"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self, state_size, action_size, buffer_size, batch_size, gamma, tau):\n",
        "         # Actor Network and Target Network\n",
        "        self.actor = Actor(state_size, action_size).to(device)\n",
        "        self.actor_target = Actor(state_size, action_size).to(device)\n",
        "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=1e-3)\n",
        "\n",
        "        # Critic Network and Target Network\n",
        "        self.critic = Critic(state_size, action_size).to(device)\n",
        "        self.critic_target = Critic(state_size, action_size).to(device)\n",
        "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=1e-3)\n",
        "        \n",
        "        # copy weights\n",
        "        self.hard_update(self.actor_target, self.actor)\n",
        "        self.hard_update(self.critic_target, self.critic)\n",
        "\n",
        "        self.noise = OUActionNoise(mu=np.zeros(action_size))\n",
        "        \n",
        "        self.memory = Memory(buffer_size, batch_size)\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.sd = 1\n",
        "        \n",
        "    def hard_update(self, target, network):\n",
        "        for target_param, param in zip(target.parameters(), network.parameters()):\n",
        "            target_param.data.copy_(param.data)\n",
        "            \n",
        "    def soft_update(self, target, network):\n",
        "        for target_param, param in zip(target.parameters(), network.parameters()):\n",
        "            target_param.data.copy_(self.tau*param.data + (1-self.tau)*target_param.data)\n",
        "            \n",
        "    def learn(self, batch):\n",
        "        \n",
        "        state, action, reward, next_state, done = batch\n",
        "\n",
        "        state = torch.tensor(state).to(device).float()\n",
        "        next_state = torch.tensor(next_state).to(device).float()\n",
        "        reward = torch.tensor(reward).to(device).float()\n",
        "        action = torch.tensor(action).to(device).float()\n",
        "        done = torch.tensor(done).to(device).int()\n",
        "        \n",
        "        # update critic\n",
        "        next_action = self.actor_target(next_state)\n",
        "\n",
        "        Q_target = self.critic_target(next_state, next_action).detach()\n",
        "        Q_target = reward.unsqueeze(1) + (self.gamma*Q_target*((1-done).unsqueeze(1)))\n",
        "\n",
        "        Q_estimate = self.critic(state, action)\n",
        "        critic_loss = F.mse_loss(Q_estimate, Q_target)        \n",
        "        \n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "        \n",
        "        # update actor\n",
        "        \n",
        "        action_prediction = self.actor(state)\n",
        "        actor_loss = -self.critic(state, action_prediction).mean()\n",
        "        \n",
        "        \n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # update actor_target and critic_target\n",
        "        \n",
        "        self.soft_update(self.critic_target, self.critic)\n",
        "        self.soft_update(self.actor_target, self.actor)\n",
        "        \n",
        "    def act(self, state, noise = True):\n",
        "        state =  torch.tensor(state).to(device).float()\n",
        "        action = self.actor(state).cpu().data.numpy()\n",
        "        noise = self.noise()\n",
        "        action_prime = action + noise#torch.tensor(self.noise(), dtype=torch.float)\n",
        "        np.clip(action_prime, -1, 1, out=action_prime)\n",
        "        return action_prime\n",
        "\n",
        "        # if noise:\n",
        "        #     noise = np.random.normal(0, self.sd)\n",
        "        #     action = action + noise\n",
        "        \n",
        "        # if action[0] > 1:\n",
        "        #     action[0] = 1\n",
        "        # if action[0] < -1:\n",
        "        #     action[0] = -1\n",
        "        # return action\n",
        "    \n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        self.memory.push((state, action, reward, next_state, done))\n",
        "        if len(self.memory) >= self.memory.batch_size:\n",
        "            self.learn(self.memory.sample())\n",
        "        \n",
        "    def save(self):\n",
        "        torch.save(self.actor, \"pendulum_actor.pkl\")\n",
        "        torch.save(self.critic, \"pendulum_critic.pkl\")\n",
        "        \n",
        "    def test(self):\n",
        "        new_env = gym.make(\"Pendulum-v1\")\n",
        "        new_env.seed(9)\n",
        "        reward = []\n",
        "        for i in range(50):\n",
        "            state = new_env.reset()\n",
        "            local_reward = 0\n",
        "            done = False\n",
        "            while not done:\n",
        "                action = self.act(state, noise = False)\n",
        "                state, r, done, _ = new_env.step(action)\n",
        "                local_reward += r\n",
        "            reward.append(local_reward)\n",
        "        return reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "size of eche action = 1\n",
            "size of state = 3\n",
            "episode: 1, current reward: -891.5267367701085\n",
            "episode: 2, current reward: -1281.5419778506214\n",
            "episode: 3, current reward: -1808.336919596742\n",
            "episode: 4, current reward: -1723.8356653091066\n",
            "episode: 5, current reward: -1509.1588451897314\n",
            "episode: 6, current reward: -1692.7218094786492\n",
            "episode: 7, current reward: -1663.0408700170638\n",
            "episode: 8, current reward: -1234.4529751234827\n",
            "episode: 9, current reward: -1515.1646069848944\n",
            "episode: 10, current reward: -1002.6326847987875\n",
            "episode: 11, current reward: -1013.6608851763325\n",
            "episode: 12, current reward: -1188.7701072575546\n",
            "episode: 13, current reward: -991.068232898093\n",
            "episode: 14, current reward: -1021.5189312858846\n",
            "episode: 15, current reward: -1293.7006657957154\n",
            "episode: 16, current reward: -1231.2184970574488\n",
            "episode: 17, current reward: -1368.4204496563323\n",
            "episode: 18, current reward: -1375.6792056464822\n",
            "episode: 19, current reward: -1382.621958849755\n",
            "episode: 20, current reward: -1380.0727260975345\n",
            "episode: 21, current reward: -1067.1476920012267\n",
            "episode: 22, current reward: -1312.0988746504995\n",
            "episode: 23, current reward: -1061.066170393112\n",
            "episode: 24, current reward: -1222.6242272896343\n",
            "episode: 25, current reward: -1190.8561239047544\n",
            "episode: 26, current reward: -1059.072131457933\n",
            "episode: 27, current reward: -1330.9279917345507\n",
            "episode: 28, current reward: -1383.5937575529763\n",
            "episode: 29, current reward: -1046.6447033433685\n",
            "episode: 30, current reward: -1181.6513290238909\n",
            "episode: 31, current reward: -1019.7622756053854\n",
            "episode: 32, current reward: -1162.0612322285556\n",
            "episode: 33, current reward: -1029.8788294690432\n",
            "episode: 34, current reward: -1021.3516104995375\n",
            "episode: 35, current reward: -889.0885473164519\n",
            "episode: 36, current reward: -916.1930756710544\n",
            "episode: 37, current reward: -888.1586579450894\n",
            "episode: 38, current reward: -950.9040942389527\n",
            "episode: 39, current reward: -1007.891053387085\n",
            "episode: 40, current reward: -1066.9231318725729\n",
            "episode: 41, current reward: -743.4559933652077\n",
            "episode: 42, current reward: -748.9221379580069\n",
            "episode: 43, current reward: -747.8948315965021\n",
            "episode: 44, current reward: -1180.8214639141445\n",
            "episode: 45, current reward: -1106.6554489282769\n",
            "episode: 46, current reward: -763.2869726368107\n",
            "episode: 47, current reward: -746.7678004070682\n",
            "episode: 48, current reward: -1202.0999303661708\n",
            "episode: 49, current reward: -1120.5810228841458\n",
            "episode: 50, current reward: -954.7659252055621\n",
            "episode: 51, current reward: -738.8299039292345\n",
            "episode: 52, current reward: -745.3022629839228\n",
            "episode: 53, current reward: -884.7743490635919\n",
            "episode: 54, current reward: -968.089361154086\n",
            "episode: 55, current reward: -880.7782142689751\n",
            "episode: 56, current reward: -904.343963171405\n",
            "episode: 57, current reward: -924.4373410088423\n",
            "episode: 58, current reward: -1108.2450246202143\n",
            "episode: 59, current reward: -869.065276125217\n",
            "episode: 60, current reward: -1073.4738348642568\n",
            "episode: 61, current reward: -833.7862504786893\n",
            "episode: 62, current reward: -978.3727538291284\n",
            "episode: 63, current reward: -763.3586949257324\n",
            "episode: 64, current reward: -1053.0725951618865\n",
            "episode: 65, current reward: -1119.6601806462525\n",
            "episode: 66, current reward: -902.2978706896968\n",
            "episode: 67, current reward: -1019.2306826479743\n",
            "episode: 68, current reward: -904.3503543898479\n",
            "episode: 69, current reward: -1047.3687489899544\n",
            "episode: 70, current reward: -1115.4604298610245\n",
            "episode: 71, current reward: -1035.1405069433824\n",
            "episode: 72, current reward: -1010.6219432915588\n",
            "episode: 73, current reward: -972.5996661064629\n",
            "episode: 74, current reward: -762.149100409145\n",
            "episode: 75, current reward: -900.722055404454\n",
            "episode: 76, current reward: -578.413761319299\n",
            "episode: 77, current reward: -767.848490988959\n",
            "episode: 78, current reward: -830.7808955062334\n",
            "episode: 79, current reward: -874.8317204803952\n",
            "episode: 80, current reward: -1040.705418377336\n",
            "episode: 81, current reward: -976.5255609666091\n",
            "episode: 82, current reward: -973.0948759323287\n",
            "episode: 83, current reward: -899.6379921318579\n",
            "episode: 84, current reward: -918.0372245114642\n",
            "episode: 85, current reward: -993.6452237420605\n",
            "episode: 86, current reward: -894.9516166469214\n",
            "episode: 87, current reward: -859.4561824034191\n",
            "episode: 88, current reward: -875.3649906606043\n",
            "episode: 89, current reward: -1038.432082138965\n",
            "episode: 90, current reward: -890.9756402799778\n",
            "episode: 91, current reward: -888.9866459456283\n",
            "episode: 92, current reward: -923.7983816993747\n",
            "episode: 93, current reward: -1105.7075581700326\n",
            "episode: 94, current reward: -830.5738234649391\n",
            "episode: 95, current reward: -947.7858193439818\n",
            "episode: 96, current reward: -973.7947093354646\n",
            "episode: 97, current reward: -890.8345272712911\n",
            "episode: 98, current reward: -988.9780322636254\n",
            "episode: 99, current reward: -1014.4997801202619\n",
            "episode: 100, current reward: -656.450305127306\n",
            "episode: 101, current reward: -769.153522595609\n",
            "episode: 102, current reward: -619.2640020819184\n",
            "episode: 103, current reward: -749.7232760392285\n",
            "episode: 104, current reward: -510.7570061484546\n",
            "episode: 105, current reward: -626.5812588244323\n",
            "episode: 106, current reward: -857.8269461586726\n",
            "episode: 107, current reward: -625.1707933961712\n",
            "episode: 108, current reward: -630.8731119785358\n",
            "episode: 109, current reward: -750.81970176046\n",
            "episode: 110, current reward: -875.1698896473051\n",
            "episode: 111, current reward: -934.825569590854\n",
            "episode: 112, current reward: -762.1126773260188\n",
            "episode: 113, current reward: -602.9069510545443\n",
            "episode: 114, current reward: -752.4804679570898\n",
            "episode: 115, current reward: -625.6600799890897\n",
            "episode: 116, current reward: -519.5796457654866\n",
            "episode: 117, current reward: -827.6248229807686\n",
            "episode: 118, current reward: -999.0920226026049\n",
            "episode: 119, current reward: -890.1916848622343\n",
            "episode: 120, current reward: -893.052264371075\n",
            "episode: 121, current reward: -900.8856716177479\n",
            "episode: 122, current reward: -730.9846054972103\n",
            "episode: 123, current reward: -717.9444171230163\n",
            "episode: 124, current reward: -766.2952691762162\n",
            "episode: 125, current reward: -987.3844914187563\n",
            "episode: 126, current reward: -640.1649008444682\n",
            "episode: 127, current reward: -963.0542320402479\n",
            "episode: 128, current reward: -738.2778887513074\n",
            "episode: 129, current reward: -743.0619720702739\n",
            "episode: 130, current reward: -710.3344086023499\n",
            "episode: 131, current reward: -891.9559434793299\n",
            "episode: 132, current reward: -1016.4965142685701\n",
            "episode: 133, current reward: -1197.4502563103483\n",
            "episode: 134, current reward: -797.4705208207757\n",
            "episode: 135, current reward: -752.8136145880355\n",
            "episode: 136, current reward: -757.9789420433947\n",
            "episode: 137, current reward: -758.210513778099\n",
            "episode: 138, current reward: -898.6607553164844\n",
            "episode: 139, current reward: -627.955696076856\n",
            "episode: 140, current reward: -718.4951624976032\n",
            "episode: 141, current reward: -747.8318729580919\n",
            "episode: 142, current reward: -744.676498933983\n",
            "episode: 143, current reward: -887.2012969379283\n",
            "episode: 144, current reward: -629.5764149602655\n",
            "episode: 145, current reward: -631.6200957337826\n",
            "episode: 146, current reward: -885.8456746627938\n",
            "episode: 147, current reward: -864.2781057793835\n",
            "episode: 148, current reward: -739.5467251000039\n",
            "episode: 149, current reward: -646.7294130315447\n",
            "episode: 150, current reward: -758.3785676649773\n",
            "episode: 151, current reward: -904.1992167461617\n",
            "episode: 152, current reward: -748.9621744854368\n",
            "episode: 153, current reward: -903.1663207829085\n",
            "episode: 154, current reward: -879.5106030666136\n",
            "episode: 155, current reward: -869.1963018873937\n",
            "episode: 156, current reward: -633.3940476201163\n",
            "episode: 157, current reward: -871.262506157857\n",
            "episode: 158, current reward: -1058.4534469125704\n",
            "episode: 159, current reward: -1100.5502897398158\n",
            "episode: 160, current reward: -516.7465631779229\n",
            "episode: 161, current reward: -746.4064845359578\n",
            "episode: 162, current reward: -1082.274990777663\n",
            "episode: 163, current reward: -807.8565767300602\n",
            "episode: 164, current reward: -628.5770287538595\n",
            "episode: 165, current reward: -501.1878878459349\n",
            "episode: 166, current reward: -745.5691816783261\n",
            "episode: 167, current reward: -736.8907845242267\n",
            "episode: 168, current reward: -903.0472115753972\n",
            "episode: 169, current reward: -506.1446812110776\n",
            "episode: 170, current reward: -901.9441938997406\n",
            "episode: 171, current reward: -748.3674263086626\n",
            "episode: 172, current reward: -907.0039032489444\n",
            "episode: 173, current reward: -941.1876862179835\n",
            "episode: 174, current reward: -871.6889843874613\n",
            "episode: 175, current reward: -653.349426646559\n",
            "episode: 176, current reward: -890.3668459344806\n",
            "episode: 177, current reward: -872.5371769079295\n",
            "episode: 178, current reward: -890.1507788524389\n",
            "episode: 179, current reward: -603.4168601408215\n",
            "episode: 180, current reward: -653.5711482680032\n",
            "episode: 181, current reward: -628.9824508754031\n",
            "episode: 182, current reward: -683.3539948884676\n",
            "episode: 183, current reward: -1058.0553364499253\n",
            "episode: 184, current reward: -769.4759875389427\n",
            "episode: 185, current reward: -620.8019654920269\n",
            "episode: 186, current reward: -746.5278674880559\n",
            "episode: 187, current reward: -881.3433997097343\n",
            "episode: 188, current reward: -1092.441227645079\n",
            "episode: 189, current reward: -1011.2518187654795\n",
            "episode: 190, current reward: -894.9956406953968\n",
            "episode: 191, current reward: -899.6285847760632\n",
            "episode: 192, current reward: -888.802443747973\n",
            "episode: 193, current reward: -915.8176636678373\n",
            "episode: 194, current reward: -748.5415736151326\n",
            "episode: 195, current reward: -834.2790381329006\n",
            "episode: 196, current reward: -732.666094827266\n",
            "episode: 197, current reward: -446.0162289267027\n",
            "episode: 198, current reward: -864.9164415626268\n",
            "episode: 199, current reward: -875.0951684592179\n",
            "episode: 200, current reward: -859.0087183005322\n",
            "episode: 201, current reward: -1023.1459108155317\n",
            "episode: 202, current reward: -621.3354284047173\n",
            "episode: 203, current reward: -736.5367020421007\n",
            "episode: 204, current reward: -607.2661109713277\n",
            "episode: 205, current reward: -934.5984047342716\n",
            "episode: 206, current reward: -1015.9646018827623\n",
            "episode: 207, current reward: -1027.1284421245864\n",
            "episode: 208, current reward: -759.2060755592236\n",
            "episode: 209, current reward: -1729.417682372306\n",
            "episode: 210, current reward: -948.9393822717878\n",
            "episode: 211, current reward: -631.4381412604885\n",
            "episode: 212, current reward: -378.97744027652993\n",
            "episode: 213, current reward: -496.637113604066\n",
            "episode: 214, current reward: -612.1659545492387\n",
            "episode: 215, current reward: -393.409561915775\n",
            "episode: 216, current reward: -506.11157045019047\n",
            "episode: 217, current reward: -969.7449641757187\n",
            "episode: 218, current reward: -894.6605147456681\n",
            "episode: 219, current reward: -727.6083736291516\n",
            "episode: 220, current reward: -622.5341656250991\n",
            "episode: 221, current reward: -612.2095600532938\n",
            "episode: 222, current reward: -879.307533195555\n",
            "episode: 223, current reward: -746.9034450307219\n",
            "episode: 224, current reward: -719.250243591642\n",
            "episode: 225, current reward: -252.36547202050215\n",
            "episode: 226, current reward: -765.9791708912938\n",
            "episode: 227, current reward: -515.2722330188022\n",
            "episode: 228, current reward: -494.2542064745958\n",
            "episode: 229, current reward: -806.1765677807957\n",
            "episode: 230, current reward: -744.7383926580819\n",
            "episode: 231, current reward: -368.2959169562444\n",
            "episode: 232, current reward: -379.112546272832\n",
            "episode: 233, current reward: -378.63084471018436\n",
            "episode: 234, current reward: -625.332461128713\n",
            "episode: 235, current reward: -379.2365415835625\n",
            "episode: 236, current reward: -600.531239168519\n",
            "episode: 237, current reward: -1094.5834499586094\n",
            "episode: 238, current reward: -616.5577227562771\n",
            "episode: 239, current reward: -604.0566866077418\n",
            "episode: 240, current reward: -907.5591801261085\n",
            "episode: 241, current reward: -854.3283588195407\n",
            "episode: 242, current reward: -624.5784253665311\n",
            "episode: 243, current reward: -530.6703977143645\n",
            "episode: 244, current reward: -1013.7326168617234\n",
            "episode: 245, current reward: -715.7642655198994\n",
            "episode: 246, current reward: -1024.1358019242753\n",
            "episode: 247, current reward: -1196.919479409577\n",
            "episode: 248, current reward: -1056.0285246729243\n",
            "episode: 249, current reward: -1024.4064190167935\n",
            "episode: 250, current reward: -845.7452175088463\n",
            "episode: 251, current reward: -1029.7253184628498\n",
            "episode: 252, current reward: -768.4047235904828\n",
            "episode: 253, current reward: -627.4538644455847\n",
            "episode: 254, current reward: -630.8645154535811\n",
            "episode: 255, current reward: -743.8652688315817\n",
            "episode: 256, current reward: -500.1553135435897\n",
            "episode: 257, current reward: -493.067284369123\n",
            "episode: 258, current reward: -488.07445007966214\n",
            "episode: 259, current reward: -622.0969071619821\n",
            "episode: 260, current reward: -768.5961125460452\n",
            "episode: 261, current reward: -755.5661972991178\n",
            "episode: 262, current reward: -599.0749935371109\n",
            "episode: 263, current reward: -509.9804125220667\n",
            "episode: 264, current reward: -734.1882901401216\n",
            "episode: 265, current reward: -765.7441793077043\n",
            "episode: 266, current reward: -499.0893097052024\n",
            "episode: 267, current reward: -377.802118113407\n",
            "episode: 268, current reward: -498.13277726426537\n",
            "episode: 269, current reward: -913.5621254842465\n",
            "episode: 270, current reward: -606.3673462068316\n",
            "episode: 271, current reward: -401.4415787711478\n",
            "episode: 272, current reward: -615.024570840911\n",
            "episode: 273, current reward: -624.9903981646645\n",
            "episode: 274, current reward: -736.856347465303\n",
            "episode: 275, current reward: -504.3722235337108\n",
            "episode: 276, current reward: -611.3787694971115\n",
            "episode: 277, current reward: -380.65701770376165\n",
            "episode: 278, current reward: -960.5535440830454\n",
            "episode: 279, current reward: -1065.6976808097936\n",
            "episode: 280, current reward: -610.5617587333082\n",
            "episode: 281, current reward: -507.89091037593755\n",
            "episode: 282, current reward: -126.98943229979851\n",
            "episode: 283, current reward: -610.2146612376029\n",
            "episode: 284, current reward: -367.8495945867944\n",
            "episode: 285, current reward: -368.06093882415365\n",
            "episode: 286, current reward: -753.0430623611443\n",
            "episode: 287, current reward: -508.8670210171384\n",
            "episode: 288, current reward: -505.4815763694529\n",
            "episode: 289, current reward: -371.7998766584134\n",
            "episode: 290, current reward: -627.9162498468979\n",
            "episode: 291, current reward: -492.0906792772418\n",
            "episode: 292, current reward: -370.96883654529034\n",
            "episode: 293, current reward: -379.9963734140393\n",
            "episode: 294, current reward: -364.8808252003575\n",
            "episode: 295, current reward: -366.6521210518494\n",
            "episode: 296, current reward: -354.3719103451759\n",
            "episode: 297, current reward: -489.6653769291044\n",
            "episode: 298, current reward: -497.13878785426425\n",
            "episode: 299, current reward: -126.37219243238731\n",
            "episode: 300, current reward: -126.10963054093529\n",
            "episode: 301, current reward: -1.1015610854019322\n",
            "episode: 302, current reward: -483.3331324437097\n",
            "episode: 303, current reward: -126.34169107981153\n",
            "episode: 304, current reward: -474.22033201912575\n",
            "episode: 305, current reward: -601.655980603235\n",
            "episode: 306, current reward: -385.0540172801006\n",
            "episode: 307, current reward: -367.50137223729365\n",
            "episode: 308, current reward: -505.2664624795201\n",
            "episode: 309, current reward: -490.24478677686216\n",
            "episode: 310, current reward: -125.9290694320674\n",
            "episode: 311, current reward: -249.42080766640203\n",
            "episode: 312, current reward: -125.87129834030026\n",
            "episode: 313, current reward: -244.98443762904463\n",
            "episode: 314, current reward: -125.15115642078649\n",
            "episode: 315, current reward: -239.95406970765075\n",
            "episode: 316, current reward: -126.73015956679865\n",
            "episode: 317, current reward: -364.2803348148549\n",
            "episode: 318, current reward: -480.46476958975353\n",
            "episode: 319, current reward: -238.76397863561542\n",
            "episode: 320, current reward: -239.7452834779221\n",
            "episode: 321, current reward: -123.3787052117915\n",
            "episode: 322, current reward: -966.0989111106505\n",
            "episode: 323, current reward: -1775.4309588211977\n",
            "episode: 324, current reward: -619.081739014943\n",
            "episode: 325, current reward: -123.5019552204486\n",
            "episode: 326, current reward: -364.13216928456364\n",
            "episode: 327, current reward: -491.98991771191726\n",
            "episode: 328, current reward: -0.9839687629124847\n",
            "episode: 329, current reward: -808.2896603123487\n",
            "episode: 330, current reward: -1175.2874925864166\n",
            "episode: 331, current reward: -474.9837242057333\n",
            "episode: 332, current reward: -0.983005120072594\n",
            "episode: 333, current reward: -611.4126492749464\n",
            "episode: 334, current reward: -122.76409477610612\n",
            "episode: 335, current reward: -1.0424653673572342\n",
            "episode: 336, current reward: -485.21372356425064\n",
            "episode: 337, current reward: -245.21085834211314\n",
            "episode: 338, current reward: -920.8040386931445\n",
            "episode: 339, current reward: -606.0777815224\n",
            "episode: 340, current reward: -245.0187466050799\n",
            "episode: 341, current reward: -251.1878350088732\n",
            "episode: 342, current reward: -925.495230571196\n",
            "episode: 343, current reward: -123.1166742366163\n",
            "episode: 344, current reward: -123.87781928300615\n",
            "episode: 345, current reward: -616.4526904418894\n",
            "episode: 346, current reward: -599.5598746778574\n",
            "episode: 347, current reward: -121.93096857733234\n",
            "episode: 348, current reward: -126.4553306129209\n",
            "episode: 349, current reward: -124.05690451938337\n",
            "episode: 350, current reward: -494.5689822352084\n",
            "episode: 351, current reward: -240.45484091744552\n",
            "episode: 352, current reward: -857.8219294886803\n",
            "episode: 353, current reward: -383.9506099874875\n",
            "episode: 354, current reward: -241.79708937142837\n",
            "episode: 355, current reward: -356.82429257129144\n",
            "episode: 356, current reward: -362.92193076795456\n",
            "episode: 357, current reward: -247.51748277421936\n",
            "episode: 358, current reward: -1.023871595508432\n",
            "episode: 359, current reward: -766.0158537290579\n",
            "episode: 360, current reward: -122.98719961742111\n",
            "episode: 361, current reward: -123.7883014596257\n",
            "episode: 362, current reward: -240.24100001216044\n",
            "episode: 363, current reward: -503.1598790992154\n",
            "episode: 364, current reward: -245.14883314097304\n",
            "episode: 365, current reward: -621.82512740974\n",
            "episode: 366, current reward: -123.8733103647789\n",
            "episode: 367, current reward: -119.11224037150619\n",
            "episode: 368, current reward: -121.91047478262489\n",
            "episode: 369, current reward: -747.1945120150485\n",
            "episode: 370, current reward: -630.9735654487004\n",
            "episode: 371, current reward: -627.912729915041\n",
            "episode: 372, current reward: -632.3620585631684\n",
            "episode: 373, current reward: -363.84240654173794\n",
            "episode: 374, current reward: -122.5461207095981\n",
            "episode: 375, current reward: -630.8897805949758\n",
            "episode: 376, current reward: -120.94234184956134\n",
            "episode: 377, current reward: -121.89546447598335\n",
            "episode: 378, current reward: -1778.563006851111\n",
            "episode: 379, current reward: -359.3315025619223\n",
            "episode: 380, current reward: -367.4212009055168\n",
            "episode: 381, current reward: -237.62812289076166\n",
            "episode: 382, current reward: -615.7645627629365\n",
            "episode: 383, current reward: -492.1919122872761\n",
            "episode: 384, current reward: -240.38956678072364\n",
            "episode: 385, current reward: -241.968464609907\n",
            "episode: 386, current reward: -1668.7920722258978\n",
            "episode: 387, current reward: -357.2391894354936\n",
            "episode: 388, current reward: -125.47675035771728\n",
            "episode: 389, current reward: -240.1714949680416\n",
            "episode: 390, current reward: -354.4892465235141\n",
            "episode: 391, current reward: -885.0048902568939\n",
            "episode: 392, current reward: -818.0977784031877\n",
            "episode: 393, current reward: -477.3428313919286\n",
            "episode: 394, current reward: -121.57022786853513\n",
            "episode: 395, current reward: -238.14501520851786\n",
            "episode: 396, current reward: -120.25516086083321\n",
            "episode: 397, current reward: -354.2174125832281\n",
            "episode: 398, current reward: -126.26216810706413\n",
            "episode: 399, current reward: -123.39998158052066\n",
            "episode: 400, current reward: -238.47340220315905\n",
            "episode: 401, current reward: -471.7876232520751\n",
            "episode: 402, current reward: -240.69495597788915\n",
            "episode: 403, current reward: -242.00771770788555\n",
            "episode: 404, current reward: -885.781482054333\n",
            "episode: 405, current reward: -124.89793863453069\n",
            "episode: 406, current reward: -242.41446575513083\n",
            "episode: 407, current reward: -123.77945996384284\n",
            "episode: 408, current reward: -353.295107964219\n",
            "episode: 409, current reward: -232.47488760795738\n",
            "episode: 410, current reward: -126.75139622609797\n",
            "episode: 411, current reward: -354.42870599650814\n",
            "episode: 412, current reward: -244.87506321096674\n",
            "episode: 413, current reward: -244.48035693756495\n",
            "episode: 414, current reward: -616.5905273439158\n",
            "episode: 415, current reward: -0.10621357476186784\n",
            "episode: 416, current reward: -358.67396655648\n",
            "episode: 417, current reward: -0.31901138254111977\n",
            "episode: 418, current reward: -231.52405131038557\n",
            "episode: 419, current reward: -123.64882840148911\n",
            "episode: 420, current reward: -483.0582541807712\n",
            "episode: 421, current reward: -121.37573211470195\n",
            "episode: 422, current reward: -122.3227526525494\n",
            "episode: 423, current reward: -122.21814139005691\n",
            "episode: 424, current reward: -492.0555421902645\n",
            "episode: 425, current reward: -239.26074204496706\n",
            "episode: 426, current reward: -124.40837991803217\n",
            "episode: 427, current reward: -1729.771469500352\n",
            "episode: 428, current reward: -702.5094743659978\n",
            "episode: 429, current reward: -243.55445169384473\n",
            "episode: 430, current reward: -238.3356008955156\n",
            "episode: 431, current reward: -481.0092479725111\n",
            "episode: 432, current reward: -119.41901581670692\n",
            "episode: 433, current reward: -1.1247257251566543\n",
            "episode: 434, current reward: -513.395332573838\n",
            "episode: 435, current reward: -239.10105719097285\n",
            "episode: 436, current reward: -123.94379615837508\n",
            "episode: 437, current reward: -366.331924875203\n",
            "episode: 438, current reward: -622.2599379503953\n",
            "episode: 439, current reward: -247.0906088195437\n",
            "episode: 440, current reward: -493.8851189532391\n",
            "episode: 441, current reward: -749.0601151382717\n",
            "episode: 442, current reward: -238.25150578123245\n",
            "episode: 443, current reward: -121.70453974724494\n",
            "episode: 444, current reward: -235.63163942769648\n",
            "episode: 445, current reward: -0.33525087641521384\n",
            "episode: 446, current reward: -0.19536104086378125\n",
            "episode: 447, current reward: -512.4754835971924\n",
            "episode: 448, current reward: -364.65507814452275\n",
            "episode: 449, current reward: -121.2348641251673\n",
            "episode: 450, current reward: -123.36277814513996\n",
            "episode: 451, current reward: -603.2168857562176\n",
            "episode: 452, current reward: -122.53517178824379\n",
            "episode: 453, current reward: -124.5543299394589\n",
            "episode: 454, current reward: -120.530573662214\n",
            "episode: 455, current reward: -627.3087639357747\n",
            "episode: 456, current reward: -359.3878349719394\n",
            "episode: 457, current reward: -120.52073440152873\n",
            "episode: 458, current reward: -0.4481497648211\n",
            "episode: 459, current reward: -241.7958683260438\n",
            "episode: 460, current reward: -370.0981301516207\n",
            "episode: 461, current reward: -645.5078749110547\n",
            "episode: 462, current reward: -484.06255296323513\n",
            "episode: 463, current reward: -238.93984727853396\n",
            "episode: 464, current reward: -243.4853942625399\n",
            "episode: 465, current reward: -249.8641861241675\n",
            "episode: 466, current reward: -125.27893847159874\n",
            "episode: 467, current reward: -122.454815983864\n",
            "episode: 468, current reward: -605.9466346018169\n",
            "episode: 469, current reward: -898.2021318463285\n",
            "episode: 470, current reward: -980.3921449639341\n",
            "episode: 471, current reward: -242.84514632627105\n",
            "episode: 472, current reward: -363.75715452220373\n",
            "episode: 473, current reward: -244.4521302888401\n",
            "episode: 474, current reward: -237.71785833873264\n",
            "episode: 475, current reward: -126.02047368771025\n",
            "episode: 476, current reward: -0.2553459825690288\n",
            "episode: 477, current reward: -488.7229086691139\n",
            "episode: 478, current reward: -244.77820282560444\n",
            "episode: 479, current reward: -122.86479487377237\n",
            "episode: 480, current reward: -243.38610662197937\n",
            "episode: 481, current reward: -0.502258390750691\n",
            "episode: 482, current reward: -119.47922406384527\n",
            "episode: 483, current reward: -121.67444036418418\n",
            "episode: 484, current reward: -123.07295123708673\n",
            "episode: 485, current reward: -123.7915780479067\n",
            "episode: 486, current reward: -597.1744181351983\n",
            "episode: 487, current reward: -484.18220221065667\n",
            "episode: 488, current reward: -514.448015442794\n",
            "episode: 489, current reward: -0.4561719689308433\n",
            "episode: 490, current reward: -119.86243608988957\n",
            "episode: 491, current reward: -633.4948026743087\n",
            "episode: 492, current reward: -125.53591684292272\n",
            "episode: 493, current reward: -243.75210864995228\n",
            "episode: 494, current reward: -244.9489774071692\n",
            "episode: 495, current reward: -128.18463664075637\n",
            "episode: 496, current reward: -242.4145479660568\n",
            "episode: 497, current reward: -497.1476085843086\n",
            "episode: 498, current reward: -123.76989281898352\n",
            "episode: 499, current reward: -518.7404140984631\n",
            "episode: 500, current reward: -478.63342747839033\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"Pendulum-v1\")\n",
        "np.random.seed(9)\n",
        "env.seed(9)\n",
        "\n",
        "action_size = env.action_space.shape[0]\n",
        "print(f'size of eche action = {action_size}')\n",
        "state_size = env.observation_space.shape[0]\n",
        "print(f'size of state = {state_size}')\n",
        "BUFFER_SIZE = int(1e6)  \n",
        "BATCH_SIZE = 64\n",
        "GAMMA = 0.99            \n",
        "TAU = 1e-2                    \n",
        "EPISODES = 500\n",
        "def ddpg(episodes):\n",
        "    agent = Agent(state_size = state_size, action_size = action_size,\n",
        "              buffer_size = BUFFER_SIZE, batch_size = BATCH_SIZE,\n",
        "              gamma = GAMMA, tau = TAU)\n",
        "    reward_list = []\n",
        "    mean_reward = -20000\n",
        "    for i in range(episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            total_reward +=reward\n",
        "            state = next_state\n",
        "            \n",
        "        reward_list.append(total_reward)\n",
        "        agent.sd = max(agent.sd - 0.01, 0.1)\n",
        "        if total_reward > 50:\n",
        "            r = agent.test()\n",
        "            local_mean = np.mean(r)\n",
        "            print(f\"episode: {i+1}, current reward: {total_reward}, max reward: {np.max(r)}, mean reward: {local_mean}\")\n",
        "            if local_mean > mean_reward:\n",
        "                mean_reward = local_mean\n",
        "                agent.save()\n",
        "                print(\"Saved\")\n",
        "        else:\n",
        "            print(f\"episode: {i+1}, current reward: {total_reward}\")\n",
        "            \n",
        "            \n",
        "    return reward_list, agent\n",
        "\n",
        "rewards, agent = ddpg(EPISODES)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_video_of_model(agent: Agent, env_name):\n",
        "    env = gym.make(env_name)\n",
        "    vid = video_recorder.VideoRecorder(env, path=\"videos/pendulum/{}_qac.mp4\".format(env_name))\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        frame = env.render(mode='rgb_array')\n",
        "        vid.capture_frame()\n",
        "        \n",
        "        action = agent.act(state)\n",
        "\n",
        "        state, reward, done, _ = env.step(action)        \n",
        "    env.close()\n",
        "\n",
        "show_video_of_model(agent, 'Pendulum-v1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
