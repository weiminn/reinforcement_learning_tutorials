{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TCxxWBZioi0N"
      },
      "source": [
        "# Advantage Actor-Critic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "oyQ7ov4M8pYR"
      },
      "outputs": [],
      "source": [
        "import sklearn.preprocessing\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import gym\n",
        "import csv\n",
        "import os\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from collections import namedtuple, deque\n",
        "import random\n",
        "import sys, time\n",
        "import argparse\n",
        "import IPython\n",
        "import os.path as osp\n",
        "import copy\n",
        "from collections import namedtuple, deque\n",
        "import random\n",
        "import json\n",
        "\n",
        "# For visualization\n",
        "from gym.wrappers.monitoring import video_recorder\n",
        "from IPython.display import HTML\n",
        "from IPython import display \n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "size of eche action = 1\n",
            "size of state = 2\n",
            "episode: 1, current reward: -53.31305762776719\n",
            "episode: 2, current reward: -51.34456695570364\n",
            "episode: 3, current reward: -48.066477787050154\n",
            "episode: 4, current reward: -50.45192757779683\n",
            "episode: 5, current reward: -50.33981897739631\n",
            "episode: 6, current reward: -48.019197666289514\n",
            "episode: 7, current reward: -48.927282760854645\n",
            "episode: 8, current reward: -48.31210700432732\n",
            "episode: 9, current reward: -46.269903312920555\n",
            "episode: 10, current reward: 69.77295245004467, max reward: -0.22800605316201913, mean reward: -0.2287922548338256\n",
            "Saved\n",
            "episode: 11, current reward: -45.75870276789966\n",
            "episode: 12, current reward: -47.23128905911216\n",
            "episode: 13, current reward: -46.56118566509315\n",
            "episode: 14, current reward: -46.394382372715675\n",
            "episode: 15, current reward: -45.74477721983888\n",
            "episode: 16, current reward: -44.08535111392763\n",
            "episode: 17, current reward: -45.3529977917509\n",
            "episode: 18, current reward: -44.83916232538853\n",
            "episode: 19, current reward: -43.137896692087764\n",
            "episode: 20, current reward: -42.787130030575874\n",
            "episode: 21, current reward: -42.4803688800094\n",
            "episode: 22, current reward: -40.61119041147408\n",
            "episode: 23, current reward: -41.754037717165886\n",
            "episode: 24, current reward: -42.86269837577666\n",
            "episode: 25, current reward: -40.020769586992316\n",
            "episode: 26, current reward: -38.97658899333281\n",
            "episode: 27, current reward: -39.9970664489142\n",
            "episode: 28, current reward: -40.29623137713337\n",
            "episode: 29, current reward: -39.67095262325013\n",
            "episode: 30, current reward: -41.52905872599953\n",
            "episode: 31, current reward: -38.12881431278393\n",
            "episode: 32, current reward: -37.82449623062018\n",
            "episode: 33, current reward: 67.0378120911334, max reward: -2.409405682766505, mean reward: -2.878263118383785\n",
            "episode: 34, current reward: -37.10856987115243\n",
            "episode: 35, current reward: 68.901859865192, max reward: -2.5097760279557804, mean reward: -2.8578187147380607\n",
            "episode: 36, current reward: -34.44586580484933\n",
            "episode: 37, current reward: 70.85887161915778, max reward: -3.459934927090355, mean reward: -3.814903530601474\n",
            "episode: 38, current reward: 68.9919391072994, max reward: -2.9450258174866364, mean reward: -3.2519335580476794\n",
            "episode: 39, current reward: 65.68213834640818, max reward: 87.22935082839443, mean reward: 65.20609909378652\n",
            "Saved\n",
            "episode: 40, current reward: 80.16074572753593, max reward: 89.72296427019914, mean reward: 84.95471848806719\n",
            "Saved\n",
            "episode: 41, current reward: 85.08067627668915, max reward: 90.00742943774173, mean reward: 88.48723783773411\n",
            "Saved\n",
            "episode: 42, current reward: 82.00704645444239, max reward: 91.21215374724014, mean reward: 90.94820344834123\n",
            "Saved\n",
            "episode: 43, current reward: 88.30812321261045, max reward: 92.02472481612521, mean reward: 91.3910267336585\n",
            "Saved\n",
            "episode: 44, current reward: 87.87894375150647, max reward: 94.75101404898273, mean reward: 92.22098538642723\n",
            "Saved\n",
            "episode: 45, current reward: 91.24536000878523, max reward: 94.5462612786029, mean reward: 91.51583395756167\n",
            "episode: 46, current reward: 92.50856656625103, max reward: 94.38707928964538, mean reward: 92.61451421054446\n",
            "Saved\n",
            "episode: 47, current reward: 94.5826176685577, max reward: 94.33170821168773, mean reward: 91.76137049153262\n",
            "episode: 48, current reward: 91.97590627760194, max reward: 89.56344351855029, mean reward: 88.75980501587789\n",
            "episode: 49, current reward: 92.55942760800787, max reward: 94.17623217477494, mean reward: 91.18610630827224\n",
            "episode: 50, current reward: 90.35687792570666, max reward: 94.08078136862711, mean reward: 90.87421972184295\n",
            "episode: 51, current reward: 92.97023507815888, max reward: 90.90001039575253, mean reward: 90.2134437464771\n",
            "episode: 52, current reward: 92.27679280963335, max reward: 93.57398465646015, mean reward: 90.42515724653498\n",
            "episode: 53, current reward: 92.43573147477079, max reward: 93.86349807330876, mean reward: 91.31428052863289\n",
            "episode: 54, current reward: 92.3765788740546, max reward: 93.7313141637251, mean reward: 90.82281845528411\n",
            "episode: 55, current reward: 92.88851347219449, max reward: 93.72616278513837, mean reward: 90.77206979482362\n",
            "episode: 56, current reward: 92.12747635223772, max reward: 93.26715062769337, mean reward: 90.43439323395651\n",
            "episode: 57, current reward: 92.62353111538792, max reward: 93.66068038225113, mean reward: 90.79605772861147\n",
            "episode: 58, current reward: 92.20201976667713, max reward: 93.68243430872741, mean reward: 90.73422987903217\n",
            "episode: 59, current reward: 92.38848446510077, max reward: 93.7418345170839, mean reward: 91.62176121000047\n",
            "episode: 60, current reward: 94.07135468517856, max reward: 93.69589039333637, mean reward: 90.98564446334481\n",
            "episode: 61, current reward: 92.67129589716826, max reward: 93.71471757049046, mean reward: 91.47109182223927\n",
            "episode: 62, current reward: 92.27714231906671, max reward: 93.7017801508432, mean reward: 91.42535812542206\n",
            "episode: 63, current reward: 93.57634854012568, max reward: 93.75406592217061, mean reward: 91.76590622231296\n",
            "episode: 64, current reward: 92.47659816218567, max reward: 93.68332497165392, mean reward: 91.64558352261982\n",
            "episode: 65, current reward: 92.5452079345574, max reward: 93.74122813748035, mean reward: 92.1639056854531\n",
            "episode: 66, current reward: 89.95265889528719, max reward: 93.73518562980055, mean reward: 92.85633721793518\n",
            "Saved\n",
            "episode: 67, current reward: 93.9754979537476, max reward: 93.73294846806992, mean reward: 92.83981767473837\n",
            "episode: 68, current reward: 92.03908559035452, max reward: 93.72684771811, mean reward: 92.66331340371109\n",
            "episode: 69, current reward: 92.13753617059352, max reward: 93.72567509964344, mean reward: 92.92797998361799\n",
            "Saved\n",
            "episode: 70, current reward: 94.53386751998073, max reward: 93.71266992359088, mean reward: 93.33443213179049\n",
            "Saved\n",
            "episode: 71, current reward: 93.91635077283755, max reward: 93.65573703258175, mean reward: 93.52883125532564\n",
            "Saved\n",
            "episode: 72, current reward: 92.34859750371254, max reward: 93.64378467168545, mean reward: 93.59856787921102\n",
            "Saved\n",
            "episode: 73, current reward: 94.41488209335697, max reward: 93.63581755653966, mean reward: 93.59371126870235\n",
            "episode: 74, current reward: 94.55207069326508, max reward: 93.5584661020087, mean reward: 93.5046993794578\n",
            "episode: 75, current reward: 94.39941113860193, max reward: 93.54077525912324, mean reward: 93.49484839655197\n",
            "episode: 76, current reward: 94.43309005402344, max reward: 93.43346533401272, mean reward: 93.38950106003638\n",
            "episode: 77, current reward: 93.86547116534719, max reward: 93.37910901845083, mean reward: 93.30132679927212\n",
            "episode: 78, current reward: 94.19108549876523, max reward: 93.30361190467995, mean reward: 93.24555472737394\n",
            "episode: 79, current reward: 94.27687398777955, max reward: 93.21835886279786, mean reward: 93.1692126183356\n",
            "episode: 80, current reward: 93.73981575688616, max reward: 93.1994773884352, mean reward: 93.1342915094899\n",
            "episode: 81, current reward: 93.27620075771637, max reward: 93.18248603334628, mean reward: 93.0800002709015\n",
            "episode: 82, current reward: 93.77078089540585, max reward: 93.00205346815828, mean reward: 92.89903525949114\n",
            "episode: 83, current reward: 93.64953751917116, max reward: 92.90503973322616, mean reward: 92.8028465982044\n",
            "episode: 84, current reward: 93.21526826054061, max reward: 92.80833836759513, mean reward: 92.66039377129896\n",
            "episode: 85, current reward: 93.33249740964166, max reward: 92.62628077125736, mean reward: 92.45359740487791\n",
            "episode: 86, current reward: 93.18649009119402, max reward: 92.70383600387277, mean reward: 92.4870253064131\n",
            "episode: 87, current reward: 93.2972447930518, max reward: 92.61251441787793, mean reward: 92.40296443424255\n",
            "episode: 88, current reward: 93.22967732916341, max reward: 92.57141754449843, mean reward: 91.8289482688511\n",
            "episode: 89, current reward: -90.55074677880236\n",
            "episode: 90, current reward: -91.55283884565155\n",
            "episode: 91, current reward: 92.53952940350693, max reward: 92.40901192103394, mean reward: 58.01347705040872\n",
            "episode: 92, current reward: -92.67304250151892\n",
            "episode: 93, current reward: 85.48667086270916, max reward: 92.82134349206466, mean reward: 89.16582783361082\n",
            "episode: 94, current reward: 86.77270719561237, max reward: 92.92540441741347, mean reward: 81.04126458674297\n",
            "episode: 95, current reward: 93.23016750540332, max reward: 92.74596694829127, mean reward: 86.02184890949293\n",
            "episode: 96, current reward: 92.77902092314801, max reward: 92.73038442475658, mean reward: 85.73834729775122\n",
            "episode: 97, current reward: 92.99793923963644, max reward: 92.72847701519487, mean reward: 87.37530428870772\n",
            "episode: 98, current reward: 87.36186384820542, max reward: 92.72536351235476, mean reward: 86.85345004110128\n",
            "episode: 99, current reward: 86.85447319715865, max reward: 92.6015403980128, mean reward: 86.73772901025227\n",
            "episode: 100, current reward: 87.26953299529109, max reward: 92.40208280512199, mean reward: 86.8255446489675\n",
            "episode: 101, current reward: 87.44266435206791, max reward: 92.64381181535599, mean reward: 86.84829811121405\n",
            "episode: 102, current reward: 87.59019807432763, max reward: 92.87302818479193, mean reward: 87.10374983616492\n",
            "episode: 103, current reward: 87.3878241768623, max reward: 92.86483426108569, mean reward: 87.28918453885916\n",
            "episode: 104, current reward: 87.92873837635545, max reward: 93.09228211277293, mean reward: 87.524013924218\n",
            "episode: 105, current reward: 87.58562902629579, max reward: 92.97376465712824, mean reward: 87.45205343707708\n",
            "episode: 106, current reward: 88.23518936887615, max reward: 93.08163089472417, mean reward: 87.52919855779295\n",
            "episode: 107, current reward: 86.92064954544779, max reward: 92.64929803771007, mean reward: 87.37817458931225\n",
            "episode: 108, current reward: 88.00470496665626, max reward: 88.8732326198602, mean reward: 87.72719351898357\n",
            "episode: 109, current reward: 88.80613743696848, max reward: 88.82836584047551, mean reward: 87.67745174692936\n",
            "episode: 110, current reward: 89.10502372283518, max reward: 91.50759546924574, mean reward: 89.96312347544084\n",
            "episode: 111, current reward: 88.96635454862721, max reward: 91.59124089871885, mean reward: 90.85242507700916\n",
            "episode: 112, current reward: 91.03389006095854, max reward: 91.50842028635134, mean reward: 90.34498177990098\n",
            "episode: 113, current reward: 91.4059711673177, max reward: 91.25970214080012, mean reward: 90.79585790958487\n",
            "episode: 114, current reward: 91.42090206581786, max reward: 91.02887909273531, mean reward: 90.68336095632893\n",
            "episode: 115, current reward: 84.43719514658709, max reward: 90.5821659513066, mean reward: 90.41160295117113\n",
            "episode: 116, current reward: 90.88769230936194, max reward: 90.61221146965457, mean reward: 89.4975492107406\n",
            "episode: 117, current reward: 91.19065584480587, max reward: 90.5042569676849, mean reward: 89.3715897668752\n",
            "episode: 118, current reward: 90.98895079223007, max reward: 90.36144458971643, mean reward: 88.20847122986997\n",
            "episode: 119, current reward: 90.90432039185288, max reward: 90.24406493609746, mean reward: 88.091047798381\n",
            "episode: 120, current reward: 84.10665681533641, max reward: 90.15931895121797, mean reward: 86.50395403571142\n",
            "episode: 121, current reward: 90.54609990360633, max reward: 90.13638738902252, mean reward: 85.51629149525829\n",
            "episode: 122, current reward: 90.42223251844405, max reward: 90.0078625434761, mean reward: 84.87157280907793\n",
            "episode: 123, current reward: -92.43563734023647\n",
            "episode: 124, current reward: -92.80939633270579\n",
            "episode: 125, current reward: -92.07054121613923\n",
            "episode: 126, current reward: -92.91507995751353\n",
            "episode: 127, current reward: -92.04852528050448\n",
            "episode: 128, current reward: -91.60996860962251\n",
            "episode: 129, current reward: -92.72166573990377\n",
            "episode: 130, current reward: -92.15071881200605\n",
            "episode: 131, current reward: 47.35460432509944\n",
            "episode: 132, current reward: -93.02510217838594\n",
            "episode: 133, current reward: -91.95467563788237\n",
            "episode: 134, current reward: -92.14896357934819\n",
            "episode: 135, current reward: -92.61233481698044\n",
            "episode: 136, current reward: -91.85167809272238\n",
            "episode: 137, current reward: -92.24119493142261\n",
            "episode: 138, current reward: -92.09543097418704\n",
            "episode: 139, current reward: 76.24007469653776, max reward: 90.59659810268039, mean reward: 90.30597716767103\n",
            "episode: 140, current reward: 91.04666931517453, max reward: 90.33267699453238, mean reward: 89.74145362576446\n",
            "episode: 141, current reward: 90.48605807304975, max reward: 90.18785639704365, mean reward: 89.48771055205181\n",
            "episode: 142, current reward: 90.37469794768536, max reward: 90.30268747457608, mean reward: 89.61191212961621\n",
            "episode: 143, current reward: 90.7849196088861, max reward: 90.47500827476847, mean reward: 89.562170295786\n",
            "episode: 144, current reward: 90.1819725873987, max reward: 90.58935585610423, mean reward: 89.58534426961374\n",
            "episode: 145, current reward: 90.87726561259177, max reward: 90.71429687985125, mean reward: 89.70419395241288\n",
            "episode: 146, current reward: 91.56033027900531, max reward: 94.0053439957274, mean reward: 91.03982953936074\n",
            "episode: 147, current reward: 90.82355526475602, max reward: 94.0234682766951, mean reward: 91.79465658620042\n",
            "episode: 148, current reward: 90.17077034354949, max reward: 94.12342595440217, mean reward: 91.03483729166928\n",
            "episode: 149, current reward: 90.76079953062904, max reward: 94.00496039170551, mean reward: 90.51066502553138\n",
            "episode: 150, current reward: 91.58160119646176, max reward: 93.86683620861524, mean reward: 90.44720372917662\n",
            "episode: 151, current reward: 91.27041694800496, max reward: 94.09713458776933, mean reward: 91.79464364539795\n",
            "episode: 152, current reward: 94.22761284914999, max reward: 94.12086514505613, mean reward: 92.44259759031642\n",
            "episode: 153, current reward: 94.20622533665447, max reward: 94.0881838980283, mean reward: 93.22295608708892\n",
            "episode: 154, current reward: 90.93919204410852, max reward: 94.14396752547796, mean reward: 93.06812011696915\n",
            "episode: 155, current reward: 94.19030110385816, max reward: 94.22279885735712, mean reward: 92.28453280670712\n",
            "episode: 156, current reward: 90.07734666662896, max reward: 94.16164172635472, mean reward: 92.91058586027364\n",
            "episode: 157, current reward: 90.92948052197984, max reward: 94.17052761154623, mean reward: 93.00078648943459\n",
            "episode: 158, current reward: 94.20885541078437, max reward: 94.09366307613705, mean reward: 93.09459963881518\n",
            "episode: 159, current reward: 94.43616059565048, max reward: 94.11608514959835, mean reward: 93.00377234463751\n",
            "episode: 160, current reward: 90.2755130430792, max reward: 94.13506409405755, mean reward: 92.74341432591875\n",
            "episode: 161, current reward: 91.8734073031841, max reward: 93.92987527456624, mean reward: 93.28086815475979\n",
            "episode: 162, current reward: 94.16736164915994, max reward: 94.03985467877946, mean reward: 92.94647374207048\n",
            "episode: 163, current reward: 90.27098799097746, max reward: 94.00458820023123, mean reward: 92.84436871699542\n",
            "episode: 164, current reward: 90.2924302905646, max reward: 93.99670130596726, mean reward: 91.69955604535548\n",
            "episode: 165, current reward: 94.06862253004336, max reward: 93.93965681611041, mean reward: 92.59283231452366\n",
            "episode: 166, current reward: 91.65534084935831, max reward: 93.83746976828672, mean reward: 93.30233933815539\n",
            "episode: 167, current reward: 93.95874515559584, max reward: 93.89622226328163, mean reward: 93.06760767962649\n",
            "episode: 168, current reward: 94.0577274641404, max reward: 93.90699167312005, mean reward: 92.71634791464679\n",
            "episode: 169, current reward: 94.01759809650471, max reward: 93.92486141035965, mean reward: 91.74406531226013\n",
            "episode: 170, current reward: 91.52553537802682, max reward: 93.90977741001151, mean reward: 91.93656729065006\n",
            "episode: 171, current reward: 94.2469947674826, max reward: 93.92428147231762, mean reward: 92.37856097915405\n",
            "episode: 172, current reward: 91.82733299563475, max reward: 93.93845201410339, mean reward: 91.86625100006378\n",
            "episode: 173, current reward: 94.1819348352104, max reward: 93.91277224300715, mean reward: 91.52956164622933\n",
            "episode: 174, current reward: 91.72135454672808, max reward: 93.92614370569521, mean reward: 91.50694797561026\n",
            "episode: 175, current reward: 91.60502421116158, max reward: 93.9229945810414, mean reward: 91.82798978144073\n",
            "episode: 176, current reward: 94.13032914643735, max reward: 93.93418972472087, mean reward: 91.7379742389019\n",
            "episode: 177, current reward: 94.25474974193408, max reward: 93.88547286182101, mean reward: 91.60008845193293\n",
            "episode: 178, current reward: 94.16828486320094, max reward: 93.84476665355079, mean reward: 91.21970136881887\n",
            "episode: 179, current reward: 91.57221408112471, max reward: 93.93992669734803, mean reward: 91.48571577698957\n",
            "episode: 180, current reward: 91.29170810263875, max reward: 93.96014064579542, mean reward: 91.48288457513823\n",
            "episode: 181, current reward: 91.48103852516184, max reward: 93.93918388796453, mean reward: 91.38605274329868\n",
            "episode: 182, current reward: 91.17442887297969, max reward: 93.65995636768496, mean reward: 91.24769703707\n",
            "episode: 183, current reward: 91.3739727433359, max reward: 93.9756154756853, mean reward: 91.66268496119886\n",
            "episode: 184, current reward: 91.70284541455196, max reward: 92.35363287911636, mean reward: 91.19030427605372\n",
            "episode: 185, current reward: 93.03192577037422, max reward: 92.59959011214315, mean reward: 91.27353125589292\n",
            "episode: 186, current reward: 91.59963563693549, max reward: 91.75921950386858, mean reward: 90.864445361659\n",
            "episode: 187, current reward: 91.72326157721201, max reward: 91.62860557134759, mean reward: 90.88645585879159\n",
            "episode: 188, current reward: 91.6691514385962, max reward: 91.78072757914798, mean reward: 90.90102881976723\n",
            "episode: 189, current reward: 91.17594532910267, max reward: 91.72334465015346, mean reward: 90.89674180986243\n",
            "episode: 190, current reward: 91.4407802889949, max reward: 91.61910289529077, mean reward: 90.90614826284497\n",
            "episode: 191, current reward: 92.20440496909646, max reward: 91.27292776132307, mean reward: 90.58779848012104\n",
            "episode: 192, current reward: 92.22341244424368, max reward: 91.384973682827, mean reward: 90.74960443036638\n",
            "episode: 193, current reward: 91.11340849492315, max reward: 91.42007889884727, mean reward: 90.74189208641225\n",
            "episode: 194, current reward: 91.82781618773349, max reward: 91.673008805793, mean reward: 90.82713780423794\n",
            "episode: 195, current reward: 91.18548941650019, max reward: 91.64676278380529, mean reward: 90.78042581597117\n",
            "episode: 196, current reward: 91.19348759336852, max reward: 90.83636402651857, mean reward: 89.96765737301843\n",
            "episode: 197, current reward: 91.21363552894816, max reward: 90.84422225083014, mean reward: 89.9783090420666\n",
            "episode: 198, current reward: 89.83881356731924, max reward: 91.16932104869205, mean reward: 90.32336764124616\n",
            "episode: 199, current reward: 91.03683539863466, max reward: 90.80830251983488, mean reward: 89.82841824569401\n",
            "episode: 200, current reward: 91.50422417414542, max reward: 90.82666729485045, mean reward: 89.80570802382321\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(Actor, self).__init__()\n",
        "        self.linear1 = nn.Linear(state_size, 32)\n",
        "        self.linear2 = nn.Linear(32, 32)\n",
        "        self.linear3 = nn.Linear(32, action_size)\n",
        "        self.reset_parameters()\n",
        "        \n",
        "    def reset_parameters(self):\n",
        "        self.linear1.weight.data.normal_(0, 1e-1)\n",
        "        self.linear2.weight.data.normal_(0, 1e-1)\n",
        "        self.linear3.weight.data.normal_(0, 1e-2)\n",
        "    \n",
        "    def forward(self, state):\n",
        "        x = state\n",
        "        x = self.linear1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.linear3(x)\n",
        "        return torch.tanh(x)\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(Critic, self).__init__()\n",
        "        self.linear1 = nn.Linear(state_size + action_size, 64)\n",
        "        self.linear2 = nn.Linear(64, 64)\n",
        "        self.linear3 = nn.Linear(64, 1)\n",
        "        self.reset_parameters()\n",
        "        \n",
        "    def reset_parameters(self):\n",
        "        self.linear1.weight.data.normal_(0, 1e-1)\n",
        "        self.linear2.weight.data.normal_(0, 1e-1)\n",
        "        self.linear3.weight.data.normal_(0, 1e-2)\n",
        "    \n",
        "    def forward(self, state, action):\n",
        "        x = torch.cat((state, action), dim=1)        \n",
        "        x = self.linear1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.linear3(x)\n",
        "        return x\n",
        "class Memory:\n",
        "    def __init__(self, buffer_size, batch_size):\n",
        "        self.buffer_size = buffer_size\n",
        "        self.batch_size = batch_size\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, element):\n",
        "        if len(self.memory) < self.buffer_size:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = element\n",
        "        self.position = (self.position + 1) % self.buffer_size\n",
        "\n",
        "    def sample(self):\n",
        "        return list(zip(*random.sample(self.memory, self.batch_size)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "class Agent:\n",
        "    def __init__(self, state_size, action_size, buffer_size, batch_size, gamma, tau):\n",
        "         # Actor Network and Target Network\n",
        "        self.actor = Actor(state_size, action_size).to(device)\n",
        "        self.actor_target = Actor(state_size, action_size).to(device)\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-3)\n",
        "\n",
        "        # Critic Network and Target Network\n",
        "        self.critic = Critic(state_size, action_size).to(device)\n",
        "        self.critic_target = Critic(state_size, action_size).to(device)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)\n",
        "        \n",
        "        # copy weights\n",
        "        self.hard_update(self.actor_target, self.actor)\n",
        "        self.hard_update(self.critic_target, self.critic)\n",
        "        \n",
        "        self.memory = Memory(buffer_size, batch_size)\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.sd = 1\n",
        "        \n",
        "    def hard_update(self, target, network):\n",
        "        for target_param, param in zip(target.parameters(), network.parameters()):\n",
        "            target_param.data.copy_(param.data)\n",
        "            \n",
        "    def soft_update(self, target, network):\n",
        "        for target_param, param in zip(target.parameters(), network.parameters()):\n",
        "            target_param.data.copy_(self.tau*param.data + (1-self.tau)*target_param.data)\n",
        "            \n",
        "    def learn(self, batch):\n",
        "        \n",
        "        state, action, reward, next_state, done = batch\n",
        "\n",
        "        state = torch.tensor(state).to(device).float()\n",
        "        next_state = torch.tensor(next_state).to(device).float()\n",
        "        reward = torch.tensor(reward).to(device).float()\n",
        "        action = torch.tensor(action).to(device)\n",
        "        done = torch.tensor(done).to(device).int()\n",
        "        \n",
        "        # update critic\n",
        "        next_action = self.actor_target(next_state)\n",
        "\n",
        "        Q_target = self.critic_target(next_state, next_action).detach()\n",
        "        Q_target = reward.unsqueeze(1) + (self.gamma*Q_target*((1-done).unsqueeze(1)))\n",
        "\n",
        "        \n",
        "        critic_loss = F.mse_loss(self.critic(state, action), Q_target)        \n",
        "        \n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "        \n",
        "        # update actor\n",
        "        \n",
        "        action_prediction = self.actor(state)\n",
        "        actor_loss = -self.critic(state, action_prediction).mean()\n",
        "        \n",
        "        \n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # update actor_target and critic_target\n",
        "        \n",
        "        self.soft_update(self.critic_target, self.critic)\n",
        "        self.soft_update(self.actor_target, self.actor)\n",
        "        \n",
        "    def act(self, state, noise = True):\n",
        "        state =  torch.tensor(state).to(device).float()\n",
        "        action = self.actor(state).cpu().data.numpy()\n",
        "        \n",
        "        if noise:\n",
        "            noise = np.random.normal(0, self.sd)\n",
        "            action = action + noise\n",
        "        \n",
        "        if action[0] > 1:\n",
        "            action[0] = 1\n",
        "        if action[0] < -1:\n",
        "            action[0] = -1\n",
        "        return action\n",
        "    \n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        self.memory.push((state, action, reward, next_state, done))\n",
        "        if len(self.memory) >= self.memory.batch_size:\n",
        "            self.learn(self.memory.sample())\n",
        "        \n",
        "    def save(self):\n",
        "        torch.save(self.actor, \"mc_actor.pkl\")\n",
        "        torch.save(self.critic, \"mc_critic.pkl\")\n",
        "        \n",
        "    def test(self):\n",
        "        new_env = gym.make(\"MountainCarContinuous-v0\")\n",
        "        new_env.seed(9)\n",
        "        reward = []\n",
        "        for i in range(50):\n",
        "            state = new_env.reset()\n",
        "            local_reward = 0\n",
        "            done = False\n",
        "            while not done:\n",
        "                action = self.act(state, noise = False)\n",
        "                state, r, done, _ = new_env.step(action)\n",
        "                local_reward += r\n",
        "            reward.append(local_reward)\n",
        "        return reward\n",
        "env = gym.make(\"MountainCarContinuous-v0\")\n",
        "np.random.seed(9)\n",
        "env.seed(9)\n",
        "\n",
        "action_size = env.action_space.shape[0]\n",
        "print(f'size of eche action = {action_size}')\n",
        "state_size = env.observation_space.shape[0]\n",
        "print(f'size of state = {state_size}')\n",
        "BUFFER_SIZE = int(1e6)  \n",
        "BATCH_SIZE = 64\n",
        "GAMMA = 0.99            \n",
        "TAU = 1e-2                    \n",
        "EPISODES = 200\n",
        "def ddpg(episodes):\n",
        "    agent = Agent(state_size = state_size, action_size = action_size,\n",
        "              buffer_size = BUFFER_SIZE, batch_size = BATCH_SIZE,\n",
        "              gamma = GAMMA, tau = TAU)\n",
        "    reward_list = []\n",
        "    mean_reward = -20000\n",
        "    for i in range(episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            total_reward +=reward\n",
        "            state = next_state\n",
        "            \n",
        "        reward_list.append(total_reward)\n",
        "        agent.sd = max(agent.sd - 0.01, 0.1)\n",
        "        if total_reward > 50:\n",
        "            r = agent.test()\n",
        "            local_mean = np.mean(r)\n",
        "            print(f\"episode: {i+1}, current reward: {total_reward}, max reward: {np.max(r)}, mean reward: {local_mean}\")\n",
        "            if local_mean > mean_reward:\n",
        "                mean_reward = local_mean\n",
        "                agent.save()\n",
        "                print(\"Saved\")\n",
        "        else:\n",
        "            print(f\"episode: {i+1}, current reward: {total_reward}\")\n",
        "            \n",
        "            \n",
        "    return reward_list, agent\n",
        "\n",
        "rewards, agent = ddpg(EPISODES)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_video_of_model(agent: Agent, env_name):\n",
        "    env = gym.make(env_name)\n",
        "    vid = video_recorder.VideoRecorder(env, path=\"videos/mc_cont/{}.mp4\".format(env_name))\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        frame = env.render(mode='rgb_array')\n",
        "        vid.capture_frame()\n",
        "        \n",
        "        action = agent.act(state)\n",
        "\n",
        "        state, reward, done, _ = env.step(action)        \n",
        "    env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "# if __name__ == '__main__':\n",
        "#     env = gym.make('MountainCarContinuous-v0')\n",
        "    # agent = Agent(state_size = state_size, action_size = action_size,\n",
        "    #           buffer_size = BUFFER_SIZE, batch_size = BATCH_SIZE,\n",
        "    #           gamma = GAMMA, tau = TAU)\n",
        "    # agent.load_models()\n",
        "show_video_of_model(agent, 'MountainCarContinuous-v0')\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
