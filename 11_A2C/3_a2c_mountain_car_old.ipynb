{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TCxxWBZioi0N"
      },
      "source": [
        "# Advantage Actor-Critic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oyQ7ov4M8pYR"
      },
      "outputs": [],
      "source": [
        "import sklearn.preprocessing\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import gym\n",
        "import csv\n",
        "import os\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from collections import namedtuple, deque\n",
        "import random\n",
        "import sys, time\n",
        "import argparse\n",
        "import IPython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bBQmph67MUru"
      },
      "outputs": [],
      "source": [
        "class NoiseProcess:\n",
        "    def __init__(self, action_space, theta,sigma,decay,min_sigma):\n",
        "        action_shape     = action_space.shape\n",
        "        self.theta       = theta\n",
        "        self.sigma       = sigma\n",
        "        self.sigma_decay = decay\n",
        "        self.min_sigma   = min_sigma\n",
        "\n",
        "        self.dt = 0.01\n",
        "\n",
        "        self.prev_x = np.zeros(action_shape)\n",
        "        self.mean   = np.zeros(action_shape)\n",
        "\n",
        "    def sample(self):\n",
        "        x = self.prev_x + self.theta * self.dt * (self.mean - self.prev_x) + \\\n",
        "            self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
        "\n",
        "        self.prev_x = x\n",
        "        return x\n",
        "\n",
        "    def decay(self):\n",
        "        self.sigma = max(self.min_sigma, self.sigma - self.sigma_decay)\n",
        "\n",
        "class NormalNoiseProcess:\n",
        "    def __init__(self, action_space, var, decay, min_sigma):\n",
        "        action_shape = action_space.shape\n",
        "\n",
        "        self.mean   = np.zeros(action_shape)\n",
        "        self.sigma = var\n",
        "        self.sigma_decay = decay\n",
        "        self.min_sigma = min_sigma\n",
        "\n",
        "    def sample(self):\n",
        "        return np.random.normal(loc = self.mean, scale=self.sigma, size=self.mean.shape)\n",
        "\n",
        "    def decay(self):\n",
        "        self.sigma = max(self.min_sigma, self.sigma - self.sigma_decay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "Sequence = namedtuple(\"Sequence\", \\\n",
        "                [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "\n",
        "class Memory:\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "        self.data = deque(maxlen=size)\n",
        "        self.max_entry = 0\n",
        "\n",
        "    def push(self, sequence):\n",
        "        self.data.append(sequence)\n",
        "        self.max_entry = len(self.data)\n",
        "\n",
        "    def sample(self, num_samples):\n",
        "        samples = random.sample(self.data, num_samples)\n",
        "\n",
        "        # convert to single sequence of samples for batch processing\n",
        "        s, a, r, s1, d = [], [], [], [], []\n",
        "        for sample in samples:\n",
        "            s.append(sample.state)\n",
        "            a.append(sample.action)\n",
        "            r.append([sample.reward])\n",
        "            s1.append(sample.next_state)\n",
        "            d.append([sample.done])\n",
        "\n",
        "        return Sequence(torch.tensor(s).float(),\n",
        "                        torch.tensor(a).float(),\n",
        "                        torch.tensor(r).float(),\n",
        "                        torch.tensor(s1).float(),\n",
        "                        torch.tensor(d))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Actor(torch.nn.Module):\n",
        "    def __init__(self, obs_size, action_space, l1_size=400, l2_size=300):\n",
        "        super(Actor, self).__init__()\n",
        "        self.action_space = action_space\n",
        "\n",
        "        self.layer1 = torch.nn.Linear(obs_size, l1_size)\n",
        "        self.layer2 = torch.nn.Linear(l1_size, l2_size)\n",
        "        self.layer3 = torch.nn.Linear(l2_size, action_space.shape[0])\n",
        "\n",
        "        # Initialization and batch norm ideas from\n",
        "        # https://github.com/philtabor/Youtube-Code-Repository/tree/master/ReinforcementLearning/PolicyGradient/DDPG/lunar-lander/pytorch\n",
        "        f1 = 1./np.sqrt(self.layer1.weight.data.size()[0])\n",
        "        nn.init.uniform_(self.layer1.weight.data, -f1, f1)\n",
        "        nn.init.uniform_(self.layer1.bias.data, -f1, f1)\n",
        "\n",
        "        f2 = 1./np.sqrt(self.layer2.weight.data.size()[0])\n",
        "        nn.init.uniform_(self.layer2.weight.data, -f2, f2)\n",
        "        nn.init.uniform_(self.layer2.bias.data, -f2, f2)\n",
        "\n",
        "        f3 = 0.003 # specified in the paper\n",
        "        nn.init.uniform_(self.layer3.weight.data, -f3, f3)\n",
        "        nn.init.uniform_(self.layer3.bias.data, -f3, f3)\n",
        "\n",
        "        self.action_space = action_space\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        x = self.layer3(x) # Don't use relu on last layer!\n",
        "\n",
        "        x = torch.tanh(x) * torch.from_numpy(self.action_space.high).float()\n",
        "        return x\n",
        "\n",
        "    def take_action(self, state, added_noise=None):\n",
        "        state_x = torch.from_numpy(state).float()\n",
        "        action = self.forward(state_x).detach().numpy()\n",
        "\n",
        "        if added_noise is not None:\n",
        "            action += added_noise\n",
        "\n",
        "        return action.clip(min=self.action_space.low, max=self.action_space.high) # TODO: clip action?\n",
        "\n",
        "\n",
        "class Critic(torch.nn.Module):\n",
        "    def __init__(self, obs_size, action_size, l1_size=400, l2_size=300):\n",
        "        super(Critic, self).__init__()\n",
        "        self.layer1 = torch.nn.Linear(obs_size, l1_size)\n",
        "        self.layer2 = torch.nn.Linear(l1_size+action_size, l2_size)\n",
        "        self.layer3 = torch.nn.Linear(l2_size, 1)\n",
        "\n",
        "        # Initialization and batch norm ideas from\n",
        "        # https://github.com/philtabor/Youtube-Code-Repository/tree/master/ReinforcementLearning/PolicyGradient/DDPG/lunar-lander/pytorch\n",
        "        f1 = 1./np.sqrt(self.layer1.weight.data.size()[0])\n",
        "        nn.init.uniform_(self.layer1.weight.data, -f1, f1)\n",
        "        nn.init.uniform_(self.layer1.bias.data, -f1, f1)\n",
        "\n",
        "        f2 = 1./np.sqrt(self.layer2.weight.data.size()[0])\n",
        "        nn.init.uniform_(self.layer2.weight.data, -f2, f2)\n",
        "        nn.init.uniform_(self.layer2.bias.data, -f2, f2)\n",
        "\n",
        "        f3 = 0.0003 # specified in the paper\n",
        "        nn.init.uniform_(self.layer3.weight.data, -f3, f3)\n",
        "        nn.init.uniform_(self.layer3.bias.data, -f3, f3)\n",
        "\n",
        "\n",
        "    def forward(self, x, a):\n",
        "        layer1_out = self.layer1(x)\n",
        "        layer1_bn = F.relu(layer1_out)\n",
        "\n",
        "        layer2_out = self.layer2(torch.cat([layer1_bn, a], dim=1))\n",
        "        layer2_bn = F.relu(layer2_out)\n",
        "\n",
        "        q_value = self.layer3(layer2_bn)\n",
        "\n",
        "        return q_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DDPG:\n",
        "    def __init__(self, opt):\n",
        "        self.params = opt\n",
        "        self.start_time = time.time()\n",
        "        self.training_timesteps = 0\n",
        "        self.last_mean = 1E6\n",
        "        self.last_var = 1E6\n",
        "        self.update_params()\n",
        "        self.reset()\n",
        "\n",
        "    def update_params(self):\n",
        "        # self.parameters = {\n",
        "        #     \"Environment Name\"            : self.params['env_name'],\n",
        "        #     \"MAX_EPISODES\"                : self.params['max_episodes'],\n",
        "        #     \"MEM_SIZE\"                    : self.params['mem_size'],\n",
        "        #     \"MEMORY_MIN\"                  : self.params['mem_min'],\n",
        "        #     \"BATCH_SIZE\"                  : self.params['batch_size'],\n",
        "        #     \"GAMMA\"                       : self.params['gamma'],\n",
        "        #     \"TAU\"                         : self.params['tau'],\n",
        "        #     \"LEARNING_RATE_ACTOR\"         : self.params['lr_actor'],\n",
        "        #     \"LEARNING_RATE_CRITIC\"        : self.params['lr_critic'],\n",
        "        #     \"NOISE_TYPE\"                  : self.params['noise_type'],\n",
        "        #     \"OU_NOISE_THETA\"              : self.params['ou_noise_theta'],\n",
        "        #     \"OU_NOISE_SIGMA\"              : self.params['ou_noise_sigma'],\n",
        "        #     \"NORMAL_VAR\"                  : self.params['normal_noise_var'],\n",
        "        #     \"NORMAL_DECAY\"                : self.params['normal_noise_decay'],\n",
        "        #     \"MIN_NORMAL_VAR\"              : self.params['min_normal_noise'],\n",
        "        #     \"start time\"                  : self.start_time,\n",
        "        #     \"L1_SIZE\"                     : self.params['l1_size'],\n",
        "        #     \"L2_SIZE\"                     : self.params['l2_size'],\n",
        "        #     \"OU_NOISE_SIGMA_DECAY_PER_EPS\": self.params['ou_noise_decay'],\n",
        "        #     \"MIN_OU_NOISE_SIGMA\"          : self.params['min_ou_noise_sigma'],\n",
        "        #     \"Save Freq\"                   : self.params['save_freq'],\n",
        "        #     \"Print Freq\"                  : self.params['print_freq'],\n",
        "        #     \"Save Actor Freq\"             : self.params['save_actor_freq'],\n",
        "        #     \"LastMeanError\"               : self.last_mean,\n",
        "        #     \"LastVarError\"                : self.last_var,\n",
        "        #     \"Training Timesteps\"          : self.training_timesteps,\n",
        "        #     }\n",
        "        \n",
        "\n",
        "        self.parameters = {\n",
        "            \"Environment Name\"            : self.params.env_name,\n",
        "            \"MAX_EPISODES\"                : self.params.max_episodes,\n",
        "            \"MEM_SIZE\"                    : self.params.mem_size,\n",
        "            \"MEMORY_MIN\"                  : self.params.mem_min,\n",
        "            \"BATCH_SIZE\"                  : self.params.batch_size,\n",
        "            \"GAMMA\"                       : self.params.gamma,\n",
        "            \"TAU\"                         : self.params.tau,\n",
        "            \"LEARNING_RATE_ACTOR\"         : self.params.lr_actor,\n",
        "            \"LEARNING_RATE_CRITIC\"        : self.params.lr_critic,\n",
        "            \"NOISE_TYPE\"                  : self.params.noise_type,\n",
        "            \"OU_NOISE_THETA\"              : self.params.ou_noise_theta,\n",
        "            \"OU_NOISE_SIGMA\"              : self.params.ou_noise_sigma,\n",
        "            \"NORMAL_VAR\"                  : self.params.normal_noise_var,\n",
        "            \"NORMAL_DECAY\"                : self.params.normal_noise_decay,\n",
        "            \"MIN_NORMAL_VAR\"              : self.params.min_normal_noise,\n",
        "            \"start time\"                  : self.start_time,\n",
        "            \"L1_SIZE\"                     : self.params.l1_size,\n",
        "            \"L2_SIZE\"                     : self.params.l2_size,\n",
        "            \"OU_NOISE_SIGMA_DECAY_PER_EPS\": self.params.ou_noise_decay,\n",
        "            \"MIN_OU_NOISE_SIGMA\"          : self.params.min_ou_noise_sigma,\n",
        "            \"Save Freq\"                   : self.params.save_freq,\n",
        "            \"Print Freq\"                  : self.params.print_freq,\n",
        "            \"Save Actor Freq\"             : self.params.save_actor_freq,\n",
        "            \"LastMeanError\"               : self.last_mean,\n",
        "            \"LastVarError\"                : self.last_var,\n",
        "            \"Training Timesteps\"          : self.training_timesteps,\n",
        "            }\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        self.envname = self.parameters[\"Environment Name\"]\n",
        "        self.env = gym.make(self.parameters[\"Environment Name\"])\n",
        "        self.env.reset()\n",
        "\n",
        "        t = time.localtime()\n",
        "        if not self.params.load_from:\n",
        "            self.name_suffix = \"_\" + self.env.spec.id[0:3] +\"_\"+ str(t.tm_mon) + \"_\" + str(t.tm_mday) + \"_\" + \\\n",
        "                    str(t.tm_hour) + \"_\" + str(t.tm_min)\n",
        "        else:\n",
        "            self.name_suffix = self.params.load_from\n",
        "\n",
        "        obs_size    = self.env.observation_space.shape[0]\n",
        "        action_size = self.env.action_space.shape[0]\n",
        "\n",
        "        self.actor        = Actor(obs_size, self.env.action_space, self.params.l1_size, self.params.l2_size)\n",
        "        self.critic       = Critic(obs_size, action_size, self.params.l1_size, self.params.l2_size)\n",
        "\n",
        "        self.target_actor = Actor(obs_size, self.env.action_space, self.params.l1_size, self.params.l2_size)\n",
        "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
        "        self.target_critic= Critic(obs_size, action_size, self.params.l1_size, self.params.l2_size)\n",
        "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
        "\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), self.params.lr_actor)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), self.params.lr_critic, weight_decay=0.01)\n",
        "\n",
        "        self.memory = Memory(self.params.mem_size)\n",
        "\n",
        "        self.start_time = time.time()\n",
        "        self.solved = None\n",
        "        self.training_timesteps = 0\n",
        "\n",
        "        if self.params.noise_type == \"ou\":\n",
        "            self.noise = NoiseProcess(self.env.action_space,\n",
        "                                        self.params.ou_noise_theta,\n",
        "                                        self.params.ou_noise_sigma,\n",
        "                                        self.params.ou_noise_decay,\n",
        "                                        self.params.min_ou_noise_sigma)\n",
        "        elif self.params.noise_type == \"normal\":\n",
        "            self.noise = NormalNoiseProcess(self.env.action_space,\n",
        "                                             self.params.normal_noise_var,\n",
        "                                             self.params.normal_noise_decay,\n",
        "                                             self.params.min_normal_noise)\n",
        "        else:\n",
        "            raise(\"Invalid noise type provided\")\n",
        "\n",
        "        self.folder_name = self.params.exp_name + self.name_suffix\n",
        "\n",
        "    def fill_memory(self):\n",
        "        fill_steps = 0\n",
        "        while fill_steps < self.params.mem_min:\n",
        "            state = self.env.reset()\n",
        "            done = False\n",
        "\n",
        "            ep_steps = 0\n",
        "            while not done and ep_steps < self.env._max_episode_steps:\n",
        "                ep_steps += 1\n",
        "                noise_to_add = self.noise.sample()\n",
        "                action = self.actor.take_action(state, noise_to_add)\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "\n",
        "                self.memory.push( \\\n",
        "                    Sequence(state, action, reward, next_state, done))\n",
        "\n",
        "                state = next_state\n",
        "                fill_steps += 1\n",
        "\n",
        "    def train(self):\n",
        "        print(\"Starting job: \\n\", self.parameters)\n",
        "\n",
        "        training_episode_rewards = []\n",
        "        test_episode_rewards = {\"mean\":[], \"var\":[]}\n",
        "        actor_loss, critic_loss = torch.tensor(1E6), torch.tensor(1E6)\n",
        "\n",
        "        self.fill_memory()\n",
        "        self.training_timesteps = 0\n",
        "\n",
        "        for episode_num in range(self.params.max_episodes):\n",
        "            state = self.env.reset()\n",
        "            done = False\n",
        "            step_scores = []\n",
        "\n",
        "            ep_steps = 0\n",
        "            while not done and ep_steps < self.env._max_episode_steps:\n",
        "                ep_steps += 1\n",
        "                self.training_timesteps += 1\n",
        "                noise_to_add = self.noise.sample()\n",
        "                action = self.actor.take_action(state, noise_to_add)\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "\n",
        "                step_scores.append(float(reward))\n",
        "\n",
        "                self.memory.push( \\\n",
        "                    Sequence(state, action, reward, next_state, done))\n",
        "\n",
        "                state = next_state\n",
        "\n",
        "                if self.memory.max_entry > self.params.mem_min:\n",
        "                    actor_loss, critic_loss = self.update_networks()\n",
        "\n",
        "            training_episode_rewards.append(sum(step_scores))\n",
        "            self.noise.decay()\n",
        "\n",
        "            print(\"Episode: \", episode_num, \" / \", self.params.max_episodes,\n",
        "                  \" | Score: \", np.array(sum(step_scores)).round(4))\n",
        "\n",
        "            if episode_num % self.params.print_freq == 0:\n",
        "                average_episode_score = sum(training_episode_rewards[-self.params.print_freq:])/float(self.params.print_freq)\n",
        "                print(\"\\nEpisode: \", episode_num, \" / \", self.params.max_episodes,\n",
        "                      \" | Avg Score: \",\n",
        "                      np.array(average_episode_score).round(4),\n",
        "                      \" | Elapsed time [s]: \",\n",
        "                      round((time.time() - self.start_time), 2),\n",
        "                      )\n",
        "                print(\"Actor loss: \", actor_loss.detach().numpy().round(4).item(),\n",
        "                        \"critic_loss: \", critic_loss.detach().numpy().round(4).item())\n",
        "\n",
        "            if episode_num % self.params.save_freq == 0:\n",
        "                print(\"\\nAverage metric at iteration \", episode_num)\n",
        "                average, variance = self.compute_average_metric()\n",
        "                test_episode_rewards[\"mean\"].append(average)\n",
        "                test_episode_rewards[\"var\"].append(variance)\n",
        "\n",
        "                if episode_num%self.params.save_actor_freq == 0:\n",
        "                    self.save_experiment(\"eps_\"+str(episode_num) + \"_of_\"+str(self.params.max_episodes),\n",
        "                                                                            training_episode_rewards,\n",
        "                                                                            test_episode_rewards,\n",
        "                                                                            save_actor=True)\n",
        "                else:\n",
        "                    self.save_experiment(\"eps_\"+str(episode_num) + \"_of_\"+str(self.params.max_episodes))\n",
        "\n",
        "                self.check_if_solved(average, episode_num)\n",
        "\n",
        "\n",
        "        print(\"Finished training. Training time: \",\n",
        "                    round((time.time() - self.start_time), 2) )\n",
        "        print(\"Episode Scores: \\n\", training_episode_rewards)\n",
        "        self.env.close()\n",
        "        self.save_experiment(\"eps_\"+str(self.params.max_episodes) + \"_of_\"+str(self.params.max_episodes),\n",
        "                                                                training_episode_rewards,\n",
        "                                                                test_episode_rewards,\n",
        "                                                                save_actor=True, save_critic=True, )\n",
        "\n",
        "        return True\n",
        "\n",
        "    def check_if_solved(self, average, episode_num):\n",
        "        solved = True\n",
        "        if \"mountaincar\" in self.env.spec.id.lower() and average > 90.0:\n",
        "            solved = True\n",
        "\n",
        "        elif \"lunarlander\" in self.env.spec.id.lower() and average > 200.0:\n",
        "            solved = True\n",
        "\n",
        "        elif \"bipedal\" in self.env.spec.id.lower() and average > 300.0:\n",
        "            solved = True\n",
        "\n",
        "        if solved and self.solved is None:\n",
        "            self.solved = episode_num\n",
        "            print(self.env.spec.id, \"solved after \", self.solved)\n",
        "\n",
        "        return solved\n",
        "\n",
        "    # mini-batch sample and update networks\n",
        "    def update_networks(self):\n",
        "        batch = self.memory.sample(self.params.batch_size)\n",
        "\n",
        "        with torch.no_grad(): # Don't need gradient for target networks\n",
        "            target_q = batch.reward + self.params.gamma * torch.mul(\\\n",
        "                                self.target_critic(batch.next_state,\n",
        "                                    self.target_actor(batch.next_state)), (~batch.done).float()).detach()\n",
        "\n",
        "        critic_q = self.critic(batch.state, batch.action)\n",
        "        critic_loss = F.mse_loss(critic_q, target_q)\n",
        "\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        actor_loss = -self.critic(batch.state, self.actor(batch.state)).mean() # gradient ascent for highest Q value\n",
        "\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # soft update\n",
        "        for param, target_param in zip(self.critic.parameters(), self.target_critic.parameters()):\n",
        "            target_param.data.copy_(self.params.tau * param.data + (1 - self.params.tau) * target_param.data)\n",
        "\n",
        "        for param, target_param in zip(self.actor.parameters(), self.target_actor.parameters()):\n",
        "            target_param.data.copy_(self.params.tau * param.data + (1 - self.params.tau) * target_param.data)\n",
        "\n",
        "        return actor_loss, critic_loss\n",
        "\n",
        "    def compute_average_metric(self):\n",
        "        num_to_test = 25\n",
        "        rewards = np.zeros(num_to_test)\n",
        "\n",
        "        for demo_ind in range(num_to_test):\n",
        "            rewards[demo_ind] = self.demonstrate()\n",
        "\n",
        "        print(\"Evaluation over \", num_to_test, \"episodes.\\n\\t\",\n",
        "                                \" Mean: \", rewards.mean(),\n",
        "                                \" | Variance: \", rewards.var())\n",
        "        self.last_mean = rewards.mean()\n",
        "        self.last_var = rewards.var()\n",
        "\n",
        "        return rewards.mean(), rewards.var()\n",
        "\n",
        "    def demonstrate(self):\n",
        "        state = self.env.reset()\n",
        "        done = False\n",
        "        rewards = 0.0\n",
        "\n",
        "        ep_steps = 0\n",
        "        while not done and ep_steps < self.env._max_episode_steps:\n",
        "            ep_steps += 1\n",
        "\n",
        "            action = self.actor.take_action(state, None)\n",
        "            next_state, reward, done, _ = self.env.step(action)\n",
        "            rewards += reward\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        self.env.reset()\n",
        "        return rewards\n",
        "\n",
        "    def save_experiment(self, experiment_name,\n",
        "                            training_episode_rewards=None,\n",
        "                            test_episode_rewards=None,\n",
        "                            save_actor=False,\n",
        "                            save_critic=False):\n",
        "\n",
        "        self.update_params()\n",
        "        experiment_name = experiment_name + \"_\" + self.params.exp_name + self.name_suffix\n",
        "\n",
        "        if self.folder_name not in os.listdir(\"experiments/\"):\n",
        "            os.mkdir(\"experiments/\" + self.folder_name)\n",
        "            print(\"made directory: \")\n",
        "        save_location = \"experiments/\" + self.folder_name + \"/\" + experiment_name\n",
        "\n",
        "        if save_actor:\n",
        "            torch.save(self.actor.state_dict(), save_location + \"actor\")\n",
        "        if save_critic:\n",
        "            torch.save(self.critic.state_dict(), save_location + \"critic\")\n",
        "\n",
        "        with open(save_location  + \".csv\", \"w\") as file:\n",
        "            w = csv.writer(file)\n",
        "            for key, val in self.parameters.items():\n",
        "                w.writerow([key, val, \"\\n\"])\n",
        "\n",
        "        if training_episode_rewards is not None:\n",
        "            np.save(save_location + \"_train_rewards\", training_episode_rewards)\n",
        "\n",
        "        if test_episode_rewards is not None:\n",
        "            np.save(save_location + \"_test_rewards_mean\", test_episode_rewards[\"mean\"])\n",
        "            np.save(save_location + \"_test_rewards_var\", test_episode_rewards[\"var\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device is  cpu\n",
            "Starting job: \n",
            " {'Environment Name': 'MountainCarContinuous-v0', 'MAX_EPISODES': 25000, 'MEM_SIZE': 100000, 'MEMORY_MIN': 1000, 'BATCH_SIZE': 64, 'GAMMA': 0.99, 'TAU': 0.001, 'LEARNING_RATE_ACTOR': 0.0001, 'LEARNING_RATE_CRITIC': 0.001, 'NOISE_TYPE': 'ou', 'OU_NOISE_THETA': 0.15, 'OU_NOISE_SIGMA': 0.2, 'NORMAL_VAR': 0.2, 'NORMAL_DECAY': 0.0, 'MIN_NORMAL_VAR': 0.2, 'start time': 1686133551.9442987, 'L1_SIZE': 400, 'L2_SIZE': 300, 'OU_NOISE_SIGMA_DECAY_PER_EPS': 0.0, 'MIN_OU_NOISE_SIGMA': 0.15, 'Save Freq': 10.0, 'Print Freq': 50, 'Save Actor Freq': 500, 'LastMeanError': 1000000.0, 'LastVarError': 1000000.0, 'Training Timesteps': 0}\n",
            "Episode:  0  /  25000  | Score:  -3.05\n",
            "\n",
            "Episode:  0  /  25000  | Avg Score:  -0.061  | Elapsed time [s]:  57.73\n",
            "Actor loss:  0.014299999922513962 critic_loss:  9.999999747378752e-05\n",
            "\n",
            "Average metric at iteration  0\n",
            "Evaluation over  25 episodes.\n",
            "\t  Mean:  -0.10855104856245643  | Variance:  4.5942089073653236e-10\n",
            "made directory: \n",
            "MountainCarContinuous-v0 solved after  0\n",
            "Episode:  1  /  25000  | Score:  -22.7397\n"
          ]
        }
      ],
      "source": [
        "class Struct:\n",
        "    def __init__(self, **entries):\n",
        "        self.__dict__.update(entries)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    opt = {\n",
        "            \"env_name\": 'MountainCarContinuous-v0',\n",
        "            \"exp_name\": 'MountainCar',\n",
        "            \"max_episodes\": 25000,\n",
        "            \"mem_size\": 100000,\n",
        "            \"mem_min\": 1000,\n",
        "            \"batch_size\": 64,\n",
        "            \"gamma\": 0.99,\n",
        "            \"tau\": 0.001,\n",
        "            \"lr_actor\": 1e-4,\n",
        "            \"lr_critic\": 1e-3,\n",
        "            \"l1_size\": 400,\n",
        "            \"l2_size\": 300,\n",
        "            \"noise_type\": 'ou',\n",
        "            \"ou_noise_theta\": 0.15,\n",
        "            \"ou_noise_sigma\": 0.2,\n",
        "            \"normal_noise_var\": 0.2,\n",
        "            \"normal_noise_decay\": 0.,\n",
        "            \"min_normal_noise\": 0.2,\n",
        "            \"ou_noise_decay\": 0.,\n",
        "            \"min_ou_noise_sigma\": 0.15,\n",
        "            \"save_freq\": 10.,\n",
        "            \"print_freq\": 50,\n",
        "            \"save_actor_freq\": 500,\n",
        "            'load_from': False\n",
        "            }\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Device is \", device)\n",
        "\n",
        "    ddpg = DDPG(Struct(**opt))\n",
        "    ddpg.train()\n",
        "\n",
        "    # if not opt.load_from:\n",
        "    #     ddpg.train()\n",
        "\n",
        "    # else:\n",
        "    #     # Use this to save or load networks. Assumes you are loading from experiments/ subdirectory.\n",
        "    #     # Example Usage:\n",
        "    #     # $ python ddpg.py --load_from quicklunarlander/finished_quick0_quick_Lun_11_28_15_27\n",
        "    #     ddpg.load_experiment()\n",
        "    #     IPython.embed()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
