{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# A coding introduction to the Markov Decision Process \n",
    "\n",
    "<br><br>\n",
    "\n",
    "In this notebook we will practice the concepts learned about control tasks and Markov decision processes. In particular, we will get familiar with a software library called OpenAI Gym that will provide us with a simple interface to these tasks.\n",
    "<br><br><br>\n",
    "<div style=\"text-align:center\">\n",
    "    <b>This notebook belongs to section 2 of the course \"Reinforcement Learning: beginner to master\".</b>\n",
    "    <br><br>\n",
    "    <a href=\"https://www.udemy.com\">Reinforcement Learning: beginner to master</a> (English)\n",
    "    <br>\n",
    "    <a href=\"https://www.udemy.com\">Reinforcement Learning: de principiante a maestro</a> (Spanish)\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<table style=\"width:35%\">\n",
    "  <tr style=\"background-color: transparent\">\n",
    "    <td style=\"width: 45%\">\n",
    "        <a target=\"_parent\" href=\"https://www.evlabs.io\" style=\"float: center\">\n",
    "            <img src=\"img/evlabs-square.png\" width=\"75\"/>\n",
    "        </a> \n",
    "    </td>\n",
    "    <td valign=\"bottom\">\n",
    "        <a target=\"_parent\" href=\"https://www.youtube.com/channel/UCksRNSzWuMV5IfdrPlglqqw\">\n",
    "            <img src=\"img/YouTube.png\" width=\"35\"/>\n",
    "        </a> \n",
    "    </td>\n",
    "    <td>\n",
    "        <a target=\"_parent\" href=\"https://www.linkedin.com/company/evlabs\">\n",
    "            <img src=\"img/LinkedIn.png\" width=\"35\"/>\n",
    "        </a> \n",
    "    </td>\n",
    "    <td>\n",
    "        <a target=\"_parent\" href=\"https://twitter.com/evelabs\">\n",
    "            <img src=\"img/Twitter.png\" width=\"35\"/>\n",
    "        </a> \n",
    "    </td>\n",
    "    <td>\n",
    "        <a target=\"_parent\" href=\"https://github.com/escape-velocity-labs/\">\n",
    "            <img src=\"img/GitHub.png\" width=\"35\"/>\n",
    "        </a> \n",
    "    </td>\n",
    "\n",
    "  </tr>\n",
    "  <tr style=\"background-color: transparent\">\n",
    "    <th style=\"text-align: center; width: 70%\">Escape Velocity Labs</th>\n",
    "  </tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from envs import Maze\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick view of the Gym library:\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "Gym is a library for reinforcement learning research. It provides us with a simple interface to a large number of tasks, including\n",
    "\n",
    "- Classic control tasks (CartPole, Pendulum, MountainCar, etc)\n",
    "- Classic video games (Space Invaders, Breakout, Pong, etc)\n",
    "- Continuous control tasks\n",
    "- Robotic arm manipulation\n",
    "\n",
    "In this section we are going to get familiar with the five methods that we'll use while solving a control\n",
    "task.\n",
    "\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "\n",
    "![title](img/mdp_diagram.svg)\n",
    "\n",
    "###### Source: https://upload.wikimedia.org/wikipedia/commons/1/1b/Reinforcement_learning_diagram.svg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Making the environment: Maze()\n",
    "\n",
    "To create an environment, just pass a string with its name to the gym.make method. If the environment exists, the method returns an instance of the gym.Env class, which represents the environment of the task we are going to solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Maze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### env.reset()\n",
    "\n",
    "This method places the environment in its initial state to  and returns it so that the agent can observe it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = env.reset()\n",
    "print(f\"The new episode will start in state: {initial_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### env.render()\n",
    "\n",
    "This method generates an image that represents the current state of the environment, in the form of a np.ndarray. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frame = env.render(mode='rgb_array')\n",
    "plt.axis('off')\n",
    "plt.title(f\"State: {initial_state}\")\n",
    "plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### env.step()\n",
    "\n",
    "This method applies the action selected by the agent in the environment, to modify it. In response, the environment returns a tuple of four objects: \n",
    "\n",
    "- The next state\n",
    "- The reward obtained\n",
    "- (bool) if the task has been completed\n",
    "- any other relevant information in a python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 2\n",
    "next_state, reward, done, info = env.step(action)\n",
    "print(f\"After moving down 1 row, the agent is in state: {next_state}\")\n",
    "print(f\"After moving down 1 row, we got a reward of: {reward}\")\n",
    "print(\"After moving down 1 row, the task is\", \"\" if done else \"not\", \"finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Render the new state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = env.render(mode='rgb_array')\n",
    "plt.axis('off')\n",
    "plt.title(f\"State: {next_state}\")\n",
    "plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### env.close()\n",
    "\n",
    "It completes the task and closes the environment, releasing the associated resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maze environment: Find the exit.\n",
    "\n",
    "\n",
    "In this section we are going to familiarize ourselves with the environment that we'll use in the lessons 2 (dynamic programming), 3 (Monte Carlo methods) and 4 (temporal difference methods). This environment is perfect for learning the basics of Reinforcement Learning because:\n",
    "\n",
    "- It has few states (25)\n",
    "- Transitions between states are deterministic ($p(s', r| s, a) = 1$)\n",
    "- All rewards are the same (-1) until the episode concludes. Thus facilitating the study of the value and action-value functions\n",
    "\n",
    "Through this environment, we are going to review the concepts seen in lesson 1 (The Markov decision process):\n",
    "\n",
    "- States and state space\n",
    "- Actions and action space\n",
    "- Trajectories and episodes\n",
    "- Rewards and returns\n",
    "- Policy\n",
    "\n",
    "\n",
    "The environment is a maze of 5x5 cells, in which the goal of the agent is to find the exit, located in the lower right corner, in the cell (4,4). In the image, the exit is colored in light green.\n",
    "\n",
    "To reach the exit, the agent can take four different actions: move up, move down, move left and move right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Maze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### States and state space\n",
    "\n",
    "The states consist of a tuple of two integers, both in the range [0, 4], representing the row and column in which the agent is currently located:\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\\begin{equation}\n",
    "    s = (row, column) \\;\\\\\n",
    "    row, column \\in \\{0,1,2,3, 4\\}\n",
    "\\end{equation}\n",
    "<br>\n",
    "<br>\n",
    "The state space (set of all possible states in the task) has 25 elements (all possible combinations of rows and columns):\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "    Rows \\times Columns \\;\\\\\n",
    "    S = \\{(0, 0), (0, 1), (1, 0), ...\\}\n",
    "\\end{equation}\n",
    "\n",
    "Information about the state space is stored in the env.observation_space property. In this environment, it is of MultiDiscrete([5 5]) type, which means that it consists of two elements (rows and columns), each with 5 different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"For example, the initial state is: {env.reset()}\")\n",
    "print(f\"The space state is of type: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Actions and action space\n",
    "\n",
    "In this environment, there are four different actions and they are represented by integers:\n",
    "\n",
    "\\begin{equation}\n",
    "a \\in \\{0, 1, 2, 3\\}\n",
    "\\end{equation}\n",
    "\n",
    "- 0 -> move up\n",
    "- 1 -> move right\n",
    "- 2 -> move down\n",
    "- 3 -> move left\n",
    "\n",
    "To execute an action, simply pass it as an argument to the env.step method. Information about the action space is stored in the env.action_space property which is of Discrete(4) class. This means that in this case it only consists of an element in the range [0,4), unlike the state space seen above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"An example of a valid action is: {env.action_space.sample()}\")\n",
    "print(f\"The action state is of type: {env.action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trajectories and episodes\n",
    "\n",
    "A trajectory is the sequence generated by moving from one state to another (both arbitrary)\n",
    "\n",
    "\\begin{equation}\n",
    "  \\tau = S_0, A_0, R_1, S_1, A_1, ... R_N, S_N,\n",
    "\\end{equation}\n",
    "\n",
    "Let's generate a trajectory of 3 moves in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Maze()\n",
    "state = env.reset()\n",
    "trajectory = []\n",
    "for _ in range(3):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, extra_info = env.step(action)\n",
    "    trajectory.append([state, action, reward, done, next_state])\n",
    "    state = next_state\n",
    "env.close()\n",
    "\n",
    "print(f\"Congrats! You just generated your first trajectory:\\n{trajectory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An episode is a trajectory that goes from the initial state of the process to the final one:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\tau = S_0, A_0, R_1, S_1, A_1, ... R_T, S_T,\n",
    "\\end{equation}\n",
    "where T is the terminal state.\n",
    "\n",
    "Let's generate a whole episode in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Maze()\n",
    "state = env.reset()\n",
    "episode = []\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, extra_info = env.step(action)\n",
    "    episode.append([state, action, reward, done, next_state])\n",
    "    state = next_state\n",
    "env.close()\n",
    "\n",
    "print(f\"Congrats! You just generated your first episode:\\n{episode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rewards and returns\n",
    "\n",
    "A reward is numerical feedback that the environment generates when the agent takes an action *a* in a state *s*:\n",
    "\n",
    "\\begin{equation}\n",
    "    r = r(s, a)\n",
    "\\end{equation}\n",
    "\n",
    "Let's generate a reward from the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Maze()\n",
    "state = env.reset()\n",
    "action = env.action_space.sample()\n",
    "_, reward, _, _ = env.step(action)\n",
    "print(f\"We achieved a reward of {reward} by taking action {action} in state {state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The return associated with a moment in time *t* is the sum (discounted) of rewards that the agent obtains from that moment. We are going to calculate $G_0$, that is, the return to the beginning of the episode:\n",
    "\n",
    "\\begin{equation}\n",
    "    G_0 = R_1 + \\gamma R_2 + \\gamma^2 R_3 + ... + \\gamma^{T-1} R_T\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "Let's assume that the discount factor $\\gamma = 0.99$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Maze()\n",
    "state = env.reset()\n",
    "done = False\n",
    "gamma = 0.99\n",
    "G_0 = 0\n",
    "t = 0\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    _, reward, done, _ = env.step(action)\n",
    "    G_0 += gamma ** t * reward\n",
    "    t += 1\n",
    "env.close()\n",
    "\n",
    "print(\n",
    "    f\"\"\"It took us {t} moves to find the exit, \n",
    "    and each reward r(s,a)=-1, so the return amounts to {G_0}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Policy\n",
    "\n",
    "A policy is a function $\\pi(a|s) \\in [0, 1]$ that gives the probability of an action given the current state. The function takes the state and action as inputs and returns a float in [0,1]. \n",
    "\n",
    "Since in practice we will need to compute the probabilities of all actions, we will represent the policy as a function that takes the state as an argument and returns the probabilities associated with each of the actions. Thus, if the probabilities are:\n",
    "\n",
    "[0.5, 0.3, 0.1]\n",
    "\n",
    "we will understand that the action with index 0 has a 50% probability of being chosen, the one with index 1 has 30% and the one with index 2 has 10%.\n",
    "\n",
    "Let's code a policy function that chooses actions randomly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(state):\n",
    "    return np.array([0.25] * 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing an episode with our random policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create and reset the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Maze()\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Compute $p(a|s) \\; \\forall a \\in \\{0, 1, 2, 3\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_probabilities = random_policy(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = ('Up', 'Right', 'Down', 'Left')\n",
    "y_pos = np.arange(len(objects))\n",
    "\n",
    "plt.bar(y_pos, action_probabilities, alpha=0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('P(a|s)')\n",
    "plt.title('Random Policy')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Use the policy to play an episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "done = False\n",
    "img = plt.imshow(env.render(mode='rgb_array')) \n",
    "while not done:\n",
    "    action = np.random.choice(range(4), 1, p=action_probabilities)\n",
    "    _, _, done, _ = env.step(action)\n",
    "    img.set_data(env.render(mode='rgb_array')) \n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[1] OpenAI gym: classic control environments](https://gym.openai.com/envs/#classic_control)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
