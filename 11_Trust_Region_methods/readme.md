# Trust Region Methods

Policy Gradient methods such as REINFORCE and Advantage Actor-Critic methods are examples of line search methods: 

$$
\begin{equation}
    \begin{split}
        \theta_{t+1} \leftarrow  \theta_t + \alpha \triangledown \hat{J}(\theta_t)
    \end{split}
\end{equation}
$$

where the parameters $\theta$ are nudged (according to the on step size) iteratively to the local maximum of the estimate of policy performance $\hat{J}(\theta)$.

<!-- ## Trust Region Methods -->

Trust Regions aim to improve the sample efficiency (learn more with less samples/experience generated) and reliability of learning steps (avoid steps that degrade the performance of the policy). It creates a bound/region around the current policy and find a different policy within that region that achieves better performance than the present one.

## Kullback-Leibler Divergence

Measure how one probability distribution $P$ is different from another $Q$:

$$
\begin{equation}
    \begin{split}
        D_{KL}(P||Q) = \sum_{x \in X} P(x) \ln(\frac{P(x)}{Q(x)}).
    \end{split}
\end{equation}
$$

So in the context of RL, we can measure how one policy $\pi_1$ is different from another $\pi_2$:

$$
\begin{equation}
    \begin{split}
        D_{KL}(\pi_1||\pi_2) = \sum_{a \in A} \pi_1(a|s) \ln(\frac{\pi_1(a|s)}{\pi_2(a|s)}).
    \end{split}
\end{equation}
$$.

Thereforce, we can restrict the changes to the policies within a $\delta$-region:

$$
\begin{equation}
    \begin{split}
        D_{KL}(\pi_1||\pi_2) < \delta.
    \end{split}
\end{equation}
$$.

If the action space is continuous, we take the integral across the entire action space:

$$
\begin{equation}
    \begin{split}
        D_{KL}(\pi_1||\pi_2) = \int^{\infty}_{-\infty} \pi_1(a|s) \ln(\frac{\pi_1(a|s)}{\pi_2(a|s)}).
    \end{split}
\end{equation}
$$.

## Generalized Policy Performance via Importance Sampling

We have the generalized performance of policy $\pi_\theta$:

$$
\begin{equation}
    \begin{split}
        J(\pi_\theta) = \mathbb{E}_{\tau \sim \pi_{\theta_{old}}}[\frac{P(\tau|\pi_\theta)}{P(\tau|\pi_{\theta_{old}})} \cdot R_{\tau}]
    \end{split}
\end{equation}
$$

where $R_\tau$ is the return following trajectory $\tau$ generated by policy $\pi_\theta$, and $\frac{P(\tau|\pi_\theta)}{P(\tau|\pi_{\theta_{old}})}$ is the importance sampling ratio.

We use data from the old policy $\pi_{\theta_{old}}$ to compute gradient to get surrogate $\pi_{\theta}$:

$$
\begin{equation}
    \begin{split}
        \triangledown_\theta J(\pi_\theta) &= \mathbb{E}_{\tau \sim \pi_{\theta_{old}}}[\frac{\triangledown_\theta P(\tau|\pi_\theta)}{P(\tau|\pi_{\theta_{old}})} \cdot R_{\tau}].
    \end{split}
\end{equation}
$$

In **standard policy gradient**, the surrogate policy $\pi_{\theta}$ is the same as the old policy $\pi_{\theta_{old}}$:

$$
\begin{equation}
    \begin{split}
        \triangledown_\theta J(\pi_\theta)|_{\theta = \theta_{old}} &= \mathbb{E}_{\tau \sim \pi_{\theta_{old}}}[\frac{\triangledown_\theta P(\tau|\pi_\theta)|_{\theta_{old}}}{P(\tau|\pi_{\theta_{old}})} \cdot R_{\tau}] \\
        &= \mathbb{E}_{\tau \sim \pi_{\theta_{old}}}[\triangledown_\theta \log P(\tau|\pi_\theta)|_{\theta_{old}} \cdot R_{\tau}]
    \end{split}
\end{equation}
$$

where we are merely doing a first-order approximation (the direction of the nudge/update) of the surrogate loss, but not the appropriate learning rate $\alpha$ or the step size.

## Policy Performance Optimization using KL-divergence

The relationship between the performances of one policy $\tilde{\pi}$ and another $\pi$:

$$
\begin{equation}
    \begin{split}
        J(\pi_\theta) &= J(\pi_{\theta_{old}}) + \mathbb{E}_{s_0, a_0, \cdots \sim \pi}[\sum^{\infty}_{t=0}\gamma_t A_{\pi_{\theta_{old}}} (s_t, a_t)] \\

        &= J(\pi_{\theta_{old}}) + \sum_s \rho_{{\pi}}(s)\sum_a {\pi}(a|s)A_{\pi_{\theta_{old}}}(s,a)
        
    \end{split}
\end{equation}
$$

where the difference is the discounted sum of Advantages (estimated by old policy) of chosen actions (by new policy) throughout the expected trajectory, and $\rho_{\tilde{\pi}(s)}$ is the visitation frequency of the state $s$ under the new policy $\tilde{\pi}$ aka. the probability of finding the state at every time steps :

$$
\begin{equation}
    \begin{split}
        \rho_{{\pi_\theta}(s)} = P(s_0 = s) + \gamma P(s_1 = s) + \gamma^2 P(s_2 = s) + \cdots.
    \end{split}
\end{equation}
$$.

The visitation frequency $\rho_\pi(s)$ is very hard to compute/estimate, so we change it to use the use the current policy $\pi$ as the local approximation or *Surrogate function* for $J(\tilde{\pi})$:

$$
\begin{equation}
    \begin{split}
        L_{\pi_{\theta_{old}}}({\pi_\theta}) &= J(\pi_{\theta_{old}}) + \sum_s \rho_{{\pi_{\theta_{old}}}}(s)\sum_a {\pi_\theta}(a|s)A_{\theta_{old}}(s,a).
        
    \end{split}
\end{equation}
$$

We can estimate the maximum step that we can take while guaranteeing policy improvement:

$$
\begin{equation}
    \begin{split}
        J({\pi_\theta}) \geq L_{\pi_{\theta_{old}}}({\pi_\theta}) - CD_{KL}^{\max}(\pi_{\theta_{old}}, {\pi_\theta}) 
    \end{split}
\end{equation}
$$

where $\max$ in $D_{KL}^{\max}$ represents the sample in our update with the highest Kullback-Leibler to make sure that no value is greater than $\delta$, and $C = \frac{4\epsilon\gamma}{(1-\gamma)^2}$ but it doesn't really matter. If the new policy $\pi_\theta$ is too far from the old policy $\pi_{\theta_{old}}$, the estimated performance $L_{\pi_{\theta_{old}}}({\pi_\theta})$ will probably get bigger, but it will be moderated by the KL-divergence $CD_{KL}^{\max}(\pi_{\theta_{old}}, {\pi_\theta})$, and thus the estimation will always be bounded from above by the improved performance $J(\pi_\theta)$.

Thus, our goal is to maximize the value of the surrogate function:

$$
\begin{equation}
    \begin{split}
        \max_{\pi_\theta} L(\pi_\theta) 
        
        % &= \mathbb{E}_{\pi_{\theta_{old}}}[\frac{\pi_\theta(a|s)}{\pi_{\theta_{old}}(a|s)} \cdot A^{\pi_{\theta_{old}}}(s,a)]
    \end{split}
\end{equation}
$$

<!-- where the constraint is $\mathbb{E}_{\pi_{old}}[D_{KL}(\pi || \pi_{old})] \leq \delta$ to make sure the policies stay close. -->
where the constraint is $D^{\max}_{KL}(\pi || \pi_{old}) \leq \delta$ to make sure the policies stay close.

