<h1 id="monte-carlo-methods">Monte Carlo Methods</h1>
<p>Whereas Dynamic Programming learns the task in a iterative approach
where you learn for every state or state-action values, Monte Carlo
method learns for only the state-actions taken throughout the episode.
This requires keeping track of state-action <span
class="math inline"><em>Q</em></span> values just like <a
href="../02_Dynamic_Programming/readme.md">Policy Iteration</a>, so
Monte Carlo is useful when you have to learn <span
class="math inline"><em>v</em><sub>*</sub>(<em>s</em>)</span> or <span
class="math inline"><em>q</em><sub>*</sub>(<em>s</em>,<em>a</em>)</span>
via sampling/experience because you have no model <span
class="math inline"><em>p</em>(<em>s</em>â€²,<em>r</em>â€²|<em>s</em>,<em>a</em>)</span>.
To generate the trajectory/experience, you use policy <span
class="math inline"><em>Ï€</em></span> to select your actions throughout
the episode:</p>
<p><span
class="math display"><em>S</em><sub>0</sub>,â€†<em>A</em><sub>0</sub>,â€†<em>R</em><sub>1</sub>,â€†<em>S</em><sub>1</sub>,â€†<em>A</em><sub>1</sub>,â€†...,â€†<em>S</em><sub><em>T</em>â€…âˆ’â€…1</sub>,â€†<em>A</em><sub><em>T</em>â€…âˆ’â€…1</sub>,â€†<em>R</em><sub><em>T</em></sub>.</span></p>
<p>At the end of episode, you compute the return for every state
visited:</p>
<p><span
class="math display"><em>v</em><sub><em>Ï€</em></sub>(<em>s</em>)â€„=â€„ğ”¼<sub><em>Ï€</em></sub>[<em>G</em><sub><em>t</em></sub>|<em>S</em><sub><em>t</em></sub>=<em>s</em>],</span></p>
<p>or</p>
<p><span
class="math display"><em>q</em><sub><em>Ï€</em></sub>(<em>s</em>,<em>a</em>)â€„=â€„ğ”¼[<em>G</em><sub><em>t</em></sub>|<em>S</em><sub><em>t</em></sub>=<em>s</em>,<em>A</em><sub><em>t</em></sub>=<em>a</em>].</span></p>
<p>where <span class="math inline">$G_t = \sum_{k=0}^{T-t-1}\gamma^k
R_{t+k+1}$</span>.</p>
<p>Thereforce, the estimated value for a state <span
class="math inline"><em>s</em></span> or state action <span
class="math inline">(<em>s</em>,<em>a</em>)</span> is the average of all
the returns that the agent has collected in that state or action of the
state:</p>
<p><span class="math display">$$
\begin{equation}
V_\pi(s) = \frac{1}{N} \sum_{k=1}^N G_{s_k},
\end{equation}
$$</span></p>
<p>and</p>
<p><span class="math display">$$
\begin{equation}
Q_\pi = \frac{1}{N}\sum_{k=1}^N G_{s,a_k}.
\end{equation}
$$</span></p>
<h2 id="calculating-values">Calculating Values</h2>
<p>For a generated trajectory <span
class="math inline">(<em>S</em><sub>0</sub>,<em>A</em><sub>0</sub>,<em>R</em><sub>1</sub>,<em>S</em><sub>1</sub>,<em>A</em><sub>1</sub>,...,<em>S</em><sub><em>T</em>â€…âˆ’â€…1</sub>,<em>A</em><sub><em>T</em>â€…âˆ’â€…1</sub>,<em>R</em><sub><em>T</em></sub>)</span>,
we calculate the returns for each moment <span
class="math inline"><em>t</em></span>:</p>
<p><span
class="math display"><em>G</em><sub><em>t</em></sub>â€„=â€„<em>R</em><sub><em>t</em>â€…+â€…1</sub>â€…+â€…<em>Î³</em><em>R</em><sub><em>t</em>â€…+â€…2</sub>â€…+â€…<em>Î³</em><sup>2</sup><em>R</em><sub><em>t</em>â€…+â€…3</sub>â€…+â€…...â€…+â€…<em>Î³</em><sup><em>T</em>â€…âˆ’â€…<em>t</em>â€…âˆ’â€…1</sup><em>R</em><sub><em>T</em></sub>.</span></p>
<p>We canâ€™t use <span class="math inline"><em>V</em></span> anymore
because value iteration does not update policy <span
class="math inline"><em>Ï€</em></span> or remember state-value <span
class="math inline"><em>Q</em></span>, and solving the task requires the
formulation of policy by evaluating <strong>each individual
action</strong> in a state. So, we can only learn for <span
class="math inline"><em>Q</em></span> which already implicity learn the
model:</p>
<p><span
class="math display"><em>q</em><sub><em>Ï€</em></sub>(<em>s</em>,<em>a</em>)â€„=â€„âˆ‘<sub><em>s</em>â€²,â€†<em>r</em></sub><em>p</em>(<em>s</em>â€²,<em>r</em>|<em>s</em>,<em>a</em>)[<em>r</em>+<em>Î³</em><em>v</em>(<em>s</em>â€²)].</span></p>
<p>where as if you want to choose and action from <span
class="math inline"><em>V</em></span>, you still need to pick the action
<span class="math inline"><em>a</em></span> with the highest value, and
to do that, you need know the next state for every action a leads to and
you need to know model <span class="math inline"><em>p</em></span> to
get probability distributions for the next state <span
class="math inline"><em>s</em>â€²</span> from <span
class="math inline">(<em>s</em>,<em>a</em>)</span>.</p>
<h2 id="importance-of-exploration">Importance of Exploration</h2>
<p>Since <span
class="math inline"><em>Q</em>(<em>s</em>,<em>a</em>)</span> is an
estimate that is improved by the agent collecting experience following
unoptimal policies. Thereforce, the estimates may not be accurate
especially in the beginning of the learning and there is a chance that
the a bad estimate prevents the agent from ever choosing <span
class="math inline">(<em>s</em>,<em>a</em>)</span> that might become
optimal in the future. To prevent this, we make sure that all actions
are chosen from time to time using:</p>
<ol type="1">
<li>Exploring starts with random state <span
class="math inline"><em>S</em><sub>0</sub>â€„âˆ¼â€„<em>S</em></span> and
random action <span
class="math inline"><em>A</em><sub>0</sub>â€„âˆ¼â€„<em>A</em>(<em>S</em><sub>0</sub>)</span>
and,</li>
<li>Stochastic policies: <span
class="math inline"><em>Ï€</em>(<em>a</em>|<em>s</em>)â€„&gt;â€„0,â€†âˆ€<em>a</em>â€„âˆˆâ€„<em>A</em>(<em>s</em>)</span>.</li>
</ol>
<h3 id="stochastic-policies">Stochastic Policies</h3>
<p>You can either generate the experience with the same policy youâ€™re
trying to optimize (On-Policy), or generate experience with an
exploratory policy <span class="math inline"><em>b</em></span> different
from the one weâ€™re going to optimize.</p>
<h4 id="epsilon-greedy-policy"><span
class="math inline"><em>Ïµ</em></span>-greedy Policy</h4>
<p>You select a random action at probability <span
class="math inline"><em>Ïµ</em></span>, and select highest <span
class="math inline"><em>Q</em>(<em>s</em>,<em>a</em>)</span> at
probability <span class="math inline">1â€…âˆ’â€…<em>Ïµ</em></span>:</p>
<p><span class="math display">$$
\begin{equation}
\pi(a|s) = \begin{cases}
1 -\epsilon + \epsilon_r &amp; \text{ for } a = a^*\\
\epsilon_r &amp; \text{ for } a \neq a^*
\end{cases}
\end{equation}
$$</span></p>
<p>where <span class="math inline">$\epsilon_r =
\frac{\epsilon}{|A|}$</span>.</p>
<h4 id="off-policy-strategy">Off-Policy strategy</h4>
<p>We can use 2 seperate policies for exploration <span
class="math inline"><em>b</em>(<em>a</em>|<em>s</em>)</span> to collect
the experience/trajectory, and optimization (Target policy <span
class="math inline"><em>Ï€</em>(<em>a</em>|<em>s</em>)</span> that uses
the experience from <span class="math inline"><em>b</em></span>) to
improve towards optimal policy:</p>
<p><span class="math display">$$
\begin{equation}\pi(s) \leftarrow \argmax_a Q(s,a).
\end{equation}$$</span></p>
<p>This means that Exploratory policy has to cover all the actions that
the Target policy can take:</p>
<p><span
class="math display"><em>Ï€</em>(<em>a</em>|<em>s</em>)â€„&gt;â€„0â€„â‡’â€„<em>b</em>(<em>a</em>|<em>s</em>)â€„&gt;â€„0.</span></p>
<p>Both <span class="math inline"><em>b</em></span> and <span
class="math inline"><em>Ï€</em></span> will still be using the same
action values, but <span class="math inline"><em>b</em></span> will have
a bit more randomness in choosing the action, so the average return is
not approximated under <span class="math inline"><em>Ï€</em></span> but
under <span class="math inline"><em>b</em></span> which handles the
exploration: <span
class="math display">ğ”¼<sub><em>b</em></sub>[<em>G</em><sub><em>t</em></sub>|<em>S</em><sub><em>t</em></sub>=<em>s</em>,<em>A</em><sub><em>t</em></sub>=<em>a</em>]â€„=â€„<em>q</em><sub><em>b</em></sub>(<em>s</em>,<em>a</em>).</span></p>
<p>We need to make sure that the Exploratory policies are exploring
properly. We can use the <strong>Importance Sampling</strong>,
statistical technique to estimate the expected values of a distribution
by working with samples from another distribution:</p>
<p><span class="math display">$$
\begin{equation}W_t = \prod_{k=t}^{T-1} \frac{\pi(A_k|S_k)}{b(A_k|S_k)}
\end{equation}$$</span></p>
<p>which gives the ratio of probability of following the trajectory by
target policy and probability of following the trajectory by exploratory
policy.</p>
<p>By correcting the returns using importance sampling, we will
approximate the value under <span
class="math inline"><em>Ï€</em></span>:</p>
<p><span
class="math display">ğ”¼[<em>W</em><sub><em>t</em></sub><em>G</em><sub><em>t</em></sub>|<em>S</em><sub><em>t</em></sub>=<em>s</em>]â€„=â€„<em>v</em><sub><em>Ï€</em></sub>(<em>s</em>).</span></p>
<p>We then update the <span class="math inline"><em>Q</em></span> values
iteratively using the <em>alpha</em> approach:</p>
<p><span class="math display">$$
\begin{equation}Q(s,a) \leftarrow Q(s,a) + \frac{W_t}{C(s,a)}[G-Q(s,a)]
\end{equation}$$</span></p>
<p>where <span class="math inline">$C(s,a) = \sum_{k=1}^{N} W_k$</span>
to normalize the updates to smooth out the learning process.</p>
